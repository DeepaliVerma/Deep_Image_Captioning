{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning-based Image Captioning with Embedding Reward\n",
    "Pranshu Gupta, Deep Learning @ Georgia Institute of Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Working on:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from cs231n.image_utils import image_from_url\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Working on: \", device)\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MS-COCO data\n",
    "We will use the Microsoft COCO dataset for captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
      "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
      "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
      "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
      "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
      "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
      "idx_to_word <class 'list'> 1004\n",
      "word_to_idx <class 'dict'> 1004\n",
      "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
      "val_urls <class 'numpy.ndarray'> (40504,) <U63\n",
      "train_captions_lens <class 'numpy.ndarray'> (400135,) float64\n",
      "val_captions_lens <class 'numpy.ndarray'> (195954,) float64\n"
     ]
    }
   ],
   "source": [
    "# Load COCO data from disk; this returns a dictionary\n",
    "# We'll work with dimensionality-reduced features for this notebook, but feel\n",
    "# free to experiment with the original features by changing the flag below.\n",
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "data[\"train_captions_lens\"] = np.zeros(data[\"train_captions\"].shape[0])\n",
    "data[\"val_captions_lens\"] = np.zeros(data[\"val_captions\"].shape[0])\n",
    "for i in range(data[\"train_captions\"].shape[0]):\n",
    "    data[\"train_captions_lens\"][i] = np.nonzero(data[\"train_captions\"][i] == 2)[0][0] + 1\n",
    "for i in range(data[\"val_captions\"].shape[0]):\n",
    "    data[\"val_captions_lens\"][i] = np.nonzero(data[\"val_captions\"][i] == 2)[0][0] + 1\n",
    "\n",
    "\n",
    "# Print out all the keys and values from the data dictionary\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLEU_score(gt_caption, sample_caption):\n",
    "    \"\"\"\n",
    "    gt_caption: string, ground-truth caption\n",
    "    sample_caption: string, your model's predicted caption\n",
    "    Returns unigram BLEU score.\n",
    "    \"\"\"\n",
    "    reference = [x for x in gt_caption.split(' ') \n",
    "                 if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "    hypothesis = [x for x in sample_caption.split(' ') \n",
    "                  if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = [1])\n",
    "    return BLEUscore\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"\n",
    "    model: CaptioningRNN model\n",
    "    Prints unigram BLEU score averaged over 1000 training and val examples.\n",
    "    \"\"\"\n",
    "    BLEUscores = {}\n",
    "    for split in ['train', 'val']:\n",
    "        minibatch = sample_coco_minibatch(data, split=split, batch_size=1000)\n",
    "        gt_captions, features, urls = minibatch\n",
    "        gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n",
    "\n",
    "        sample_captions = model.sample(features)\n",
    "        sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n",
    "\n",
    "        total_score = 0.0\n",
    "        for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n",
    "            total_score += BLEU_score(gt_caption, sample_caption)\n",
    "\n",
    "        BLEUscores[split] = total_score / len(sample_captions)\n",
    "\n",
    "    for split in BLEUscores:\n",
    "        print('Average BLEU score for %s: %f' % (split, BLEUscores[split]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        \n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        \n",
    "        self.cnn2linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(wordvec_dim, hidden_dim, batch_first=True)\n",
    "        self.linear2vocab = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.probs = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        hidden_init = self.cnn2linear(features)\n",
    "        cell_init = torch.zeros_like(hidden_init)\n",
    "        output, _ = self.lstm(input_captions, (hidden_init, cell_init))\n",
    "        output = self.linear2vocab(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "policyNetwork = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(policyNetwork.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = load_coco_data(max_train=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 10.344365119934082\n",
      "epoch: 3 loss: 10.179471969604492\n",
      "epoch: 17 loss: 10.02580451965332\n",
      "epoch: 32 loss: 9.871771812438965\n",
      "epoch: 47 loss: 9.743988990783691\n",
      "epoch: 49 loss: 8.812932968139648\n",
      "epoch: 291 loss: 8.67715072631836\n",
      "epoch: 336 loss: 8.276082038879395\n",
      "epoch: 376 loss: 8.249432563781738\n",
      "epoch: 399 loss: 8.062466621398926\n",
      "epoch: 502 loss: 8.006856918334961\n",
      "epoch: 527 loss: 7.8093061447143555\n",
      "epoch: 608 loss: 7.712879180908203\n",
      "epoch: 676 loss: 7.533693790435791\n",
      "epoch: 762 loss: 7.414658546447754\n",
      "epoch: 774 loss: 7.262825965881348\n",
      "epoch: 782 loss: 7.2376885414123535\n",
      "epoch: 907 loss: 7.1637959480285645\n",
      "epoch: 911 loss: 7.10874605178833\n",
      "epoch: 938 loss: 6.934282302856445\n",
      "epoch: 980 loss: 6.9209394454956055\n",
      "epoch: 1005 loss: 6.589231491088867\n",
      "epoch: 1107 loss: 6.376235485076904\n",
      "epoch: 1117 loss: 6.227982044219971\n",
      "epoch: 1248 loss: 6.101768493652344\n",
      "epoch: 1262 loss: 5.96243143081665\n",
      "epoch: 1309 loss: 5.938260078430176\n",
      "epoch: 1428 loss: 5.467482089996338\n",
      "epoch: 1652 loss: 5.397933483123779\n",
      "epoch: 1670 loss: 5.226827144622803\n",
      "epoch: 1746 loss: 5.20128059387207\n",
      "epoch: 1771 loss: 4.899995803833008\n",
      "epoch: 1912 loss: 4.7961649894714355\n",
      "epoch: 1941 loss: 4.564793586730957\n",
      "epoch: 1981 loss: 4.454484462738037\n",
      "epoch: 2150 loss: 4.334531307220459\n",
      "epoch: 2156 loss: 4.266141891479492\n",
      "epoch: 2160 loss: 4.196115970611572\n",
      "epoch: 2222 loss: 4.05812406539917\n",
      "epoch: 2258 loss: 3.9649100303649902\n",
      "epoch: 2388 loss: 3.91683292388916\n",
      "epoch: 2505 loss: 3.888580322265625\n",
      "epoch: 2523 loss: 3.8655178546905518\n",
      "epoch: 2539 loss: 3.639010429382324\n",
      "epoch: 2589 loss: 3.5529308319091797\n",
      "epoch: 2661 loss: 3.551360845565796\n",
      "epoch: 2667 loss: 3.4776577949523926\n",
      "epoch: 2781 loss: 3.4262845516204834\n",
      "epoch: 2810 loss: 3.3861870765686035\n",
      "epoch: 2873 loss: 3.0164411067962646\n",
      "epoch: 2951 loss: 2.9630770683288574\n",
      "epoch: 3125 loss: 2.918764591217041\n",
      "epoch: 3179 loss: 2.915116310119629\n",
      "epoch: 3200 loss: 2.7191460132598877\n",
      "epoch: 3201 loss: 2.660384178161621\n",
      "epoch: 3386 loss: 2.6510605812072754\n",
      "epoch: 3419 loss: 2.579019784927368\n",
      "epoch: 3435 loss: 2.517526388168335\n",
      "epoch: 3444 loss: 2.4581358432769775\n",
      "epoch: 3611 loss: 2.3472695350646973\n",
      "epoch: 3695 loss: 2.3160717487335205\n",
      "epoch: 3747 loss: 2.229379177093506\n",
      "epoch: 3757 loss: 2.104635715484619\n",
      "epoch: 3831 loss: 2.0641705989837646\n",
      "epoch: 3904 loss: 2.032160997390747\n",
      "epoch: 3976 loss: 2.0198123455047607\n",
      "epoch: 4035 loss: 2.012235641479492\n",
      "epoch: 4045 loss: 1.9414297342300415\n",
      "epoch: 4063 loss: 1.8667720556259155\n",
      "epoch: 4091 loss: 1.7652989625930786\n",
      "epoch: 4127 loss: 1.713186264038086\n",
      "epoch: 4261 loss: 1.6235979795455933\n",
      "epoch: 4361 loss: 1.594093680381775\n",
      "epoch: 4480 loss: 1.5644792318344116\n",
      "epoch: 4565 loss: 1.5570582151412964\n",
      "epoch: 4704 loss: 1.486487865447998\n",
      "epoch: 4774 loss: 1.475583791732788\n",
      "epoch: 4854 loss: 1.4104620218276978\n",
      "epoch: 4978 loss: 1.3757439851760864\n",
      "epoch: 5017 loss: 1.3337457180023193\n",
      "epoch: 5034 loss: 1.2597612142562866\n",
      "epoch: 5157 loss: 1.254582166671753\n",
      "epoch: 5179 loss: 1.252065658569336\n",
      "epoch: 5210 loss: 1.2512874603271484\n",
      "epoch: 5258 loss: 1.2029415369033813\n",
      "epoch: 5263 loss: 1.2023755311965942\n",
      "epoch: 5288 loss: 1.1884080171585083\n",
      "epoch: 5301 loss: 1.1860692501068115\n",
      "epoch: 5319 loss: 1.0792367458343506\n",
      "epoch: 5449 loss: 1.078542709350586\n",
      "epoch: 5458 loss: 1.012381911277771\n",
      "epoch: 5477 loss: 1.0098673105239868\n",
      "epoch: 5621 loss: 1.0080077648162842\n",
      "epoch: 5651 loss: 1.0018302202224731\n",
      "epoch: 5658 loss: 0.991522490978241\n",
      "epoch: 5743 loss: 0.9418976902961731\n",
      "epoch: 5792 loss: 0.9220266938209534\n",
      "epoch: 5815 loss: 0.8631142377853394\n",
      "epoch: 5897 loss: 0.8497470617294312\n",
      "epoch: 5989 loss: 0.8423615097999573\n",
      "epoch: 6014 loss: 0.8166280388832092\n",
      "epoch: 6073 loss: 0.7749141454696655\n",
      "epoch: 6664 loss: 0.7677327394485474\n",
      "epoch: 6696 loss: 0.7576273083686829\n",
      "epoch: 6712 loss: 0.7542065978050232\n",
      "epoch: 6717 loss: 0.7064668536186218\n",
      "epoch: 6737 loss: 0.64876788854599\n",
      "epoch: 6773 loss: 0.6352921724319458\n",
      "epoch: 6813 loss: 0.6147001385688782\n",
      "epoch: 6872 loss: 0.5577423572540283\n",
      "epoch: 7057 loss: 0.5403820872306824\n",
      "epoch: 7093 loss: 0.5133659243583679\n",
      "epoch: 7214 loss: 0.4966493844985962\n",
      "epoch: 7261 loss: 0.4929063618183136\n",
      "epoch: 7276 loss: 0.4728061556816101\n",
      "epoch: 7335 loss: 0.46762314438819885\n",
      "epoch: 7470 loss: 0.4395939111709595\n",
      "epoch: 7502 loss: 0.4334039092063904\n",
      "epoch: 7621 loss: 0.42567193508148193\n",
      "epoch: 7643 loss: 0.4046348035335541\n",
      "epoch: 7666 loss: 0.3963109850883484\n",
      "epoch: 7905 loss: 0.39086997509002686\n",
      "epoch: 7996 loss: 0.38720858097076416\n",
      "epoch: 8604 loss: 0.3622758984565735\n",
      "epoch: 8666 loss: 0.36181995272636414\n",
      "epoch: 8738 loss: 0.3514693081378937\n",
      "epoch: 8745 loss: 0.31157347559928894\n",
      "epoch: 8824 loss: 0.3107045590877533\n",
      "epoch: 8857 loss: 0.3056231439113617\n",
      "epoch: 8920 loss: 0.30197662115097046\n",
      "epoch: 8944 loss: 0.29547250270843506\n",
      "epoch: 9033 loss: 0.29222920536994934\n",
      "epoch: 9055 loss: 0.29212987422943115\n",
      "epoch: 9086 loss: 0.2879317104816437\n",
      "epoch: 9141 loss: 0.2768223285675049\n",
      "epoch: 9211 loss: 0.25591596961021423\n",
      "epoch: 9225 loss: 0.2526695430278778\n",
      "epoch: 9301 loss: 0.24961787462234497\n",
      "epoch: 9389 loss: 0.24430225789546967\n",
      "epoch: 9439 loss: 0.2431091070175171\n",
      "epoch: 9444 loss: 0.2414109706878662\n",
      "epoch: 9587 loss: 0.2330975979566574\n",
      "epoch: 9681 loss: 0.22462432086467743\n",
      "epoch: 9728 loss: 0.2235383540391922\n",
      "epoch: 9741 loss: 0.22308067977428436\n",
      "epoch: 9755 loss: 0.21027691662311554\n",
      "epoch: 9775 loss: 0.2072822004556656\n",
      "epoch: 9823 loss: 0.2061530351638794\n",
      "epoch: 9906 loss: 0.19476844370365143\n",
      "epoch: 10147 loss: 0.1773490011692047\n",
      "epoch: 10394 loss: 0.17433443665504456\n",
      "epoch: 10460 loss: 0.16432006657123566\n",
      "epoch: 11479 loss: 0.1631089597940445\n",
      "epoch: 11587 loss: 0.1626053750514984\n",
      "epoch: 11634 loss: 0.16115710139274597\n",
      "epoch: 11642 loss: 0.16112422943115234\n",
      "epoch: 11649 loss: 0.15851432085037231\n",
      "epoch: 11692 loss: 0.14937609434127808\n",
      "epoch: 11714 loss: 0.13908670842647552\n",
      "epoch: 11841 loss: 0.13471029698848724\n",
      "epoch: 11955 loss: 0.13057014346122742\n",
      "epoch: 12050 loss: 0.11590248346328735\n",
      "epoch: 12545 loss: 0.11190792173147202\n",
      "epoch: 12586 loss: 0.1114283874630928\n",
      "epoch: 12593 loss: 0.10624363273382187\n",
      "epoch: 12698 loss: 0.10322131216526031\n",
      "epoch: 12766 loss: 0.10264838486909866\n",
      "epoch: 12783 loss: 0.10079672187566757\n",
      "epoch: 12863 loss: 0.09949212521314621\n",
      "epoch: 12928 loss: 0.0970076471567154\n",
      "epoch: 13053 loss: 0.09554172307252884\n",
      "epoch: 13066 loss: 0.09374464303255081\n",
      "epoch: 13190 loss: 0.09279731661081314\n",
      "epoch: 13256 loss: 0.09212561696767807\n",
      "epoch: 13300 loss: 0.08635833114385605\n",
      "epoch: 13574 loss: 0.08623295277357101\n",
      "epoch: 13575 loss: 0.08294423669576645\n",
      "epoch: 13614 loss: 0.08260439336299896\n",
      "epoch: 13697 loss: 0.0811009630560875\n",
      "epoch: 14983 loss: 0.07940694689750671\n",
      "epoch: 15208 loss: 0.07774093002080917\n",
      "epoch: 15340 loss: 0.0720517486333847\n",
      "epoch: 15496 loss: 0.07157372683286667\n",
      "epoch: 15535 loss: 0.07071810960769653\n",
      "epoch: 15682 loss: 0.06627251207828522\n",
      "epoch: 15715 loss: 0.06579700857400894\n",
      "epoch: 15808 loss: 0.06519366055727005\n",
      "epoch: 15868 loss: 0.06325649470090866\n",
      "epoch: 15986 loss: 0.06273749470710754\n",
      "epoch: 16042 loss: 0.06210201978683472\n",
      "epoch: 16102 loss: 0.06090543419122696\n",
      "epoch: 16126 loss: 0.057280607521533966\n",
      "epoch: 16154 loss: 0.05701717734336853\n",
      "epoch: 16233 loss: 0.05588013306260109\n",
      "epoch: 16336 loss: 0.05520028993487358\n",
      "epoch: 16351 loss: 0.054699502885341644\n",
      "epoch: 16511 loss: 0.05447901412844658\n",
      "epoch: 16535 loss: 0.053699325770139694\n",
      "epoch: 16671 loss: 0.053362634032964706\n",
      "epoch: 16701 loss: 0.05231761932373047\n",
      "epoch: 16708 loss: 0.05216916278004646\n",
      "epoch: 16798 loss: 0.04950997978448868\n",
      "epoch: 16912 loss: 0.049274470657110214\n",
      "epoch: 16987 loss: 0.04527732729911804\n",
      "epoch: 19039 loss: 0.043032869696617126\n",
      "epoch: 19170 loss: 0.04263666272163391\n",
      "epoch: 19333 loss: 0.04142049700021744\n",
      "epoch: 19507 loss: 0.040937792509794235\n",
      "epoch: 19537 loss: 0.03736545890569687\n",
      "epoch: 19758 loss: 0.03690120205283165\n",
      "epoch: 19790 loss: 0.03577157482504845\n",
      "epoch: 20080 loss: 0.03427950665354729\n",
      "epoch: 20177 loss: 0.03367752581834793\n",
      "epoch: 20190 loss: 0.03286437690258026\n",
      "epoch: 20295 loss: 0.03282545506954193\n",
      "epoch: 20358 loss: 0.032718345522880554\n",
      "epoch: 20504 loss: 0.03129177540540695\n",
      "epoch: 20608 loss: 0.030736004933714867\n",
      "epoch: 20800 loss: 0.0307109784334898\n",
      "epoch: 22897 loss: 0.03047901764512062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22905 loss: 0.029251957312226295\n",
      "epoch: 23273 loss: 0.02822219766676426\n",
      "epoch: 23323 loss: 0.027365315705537796\n",
      "epoch: 23413 loss: 0.02725229226052761\n",
      "epoch: 23515 loss: 0.026646923273801804\n",
      "epoch: 23694 loss: 0.025608716532588005\n",
      "epoch: 23754 loss: 0.024744512513279915\n",
      "epoch: 23820 loss: 0.023553505539894104\n",
      "epoch: 24102 loss: 0.023297669366002083\n",
      "epoch: 24267 loss: 0.023097923025488853\n",
      "epoch: 24341 loss: 0.022959310561418533\n",
      "epoch: 24415 loss: 0.02175622060894966\n",
      "epoch: 24599 loss: 0.020215949043631554\n",
      "epoch: 27552 loss: 0.01901116780936718\n",
      "epoch: 28024 loss: 0.01788322627544403\n",
      "epoch: 28083 loss: 0.01761564053595066\n",
      "epoch: 28290 loss: 0.017190484330058098\n",
      "epoch: 28305 loss: 0.01714828424155712\n",
      "epoch: 28412 loss: 0.015786800533533096\n",
      "epoch: 31763 loss: 0.015440654009580612\n",
      "epoch: 31922 loss: 0.014904175885021687\n",
      "epoch: 32094 loss: 0.014722405932843685\n",
      "epoch: 32207 loss: 0.01450385246425867\n",
      "epoch: 34512 loss: 0.014189643785357475\n",
      "epoch: 34744 loss: 0.014027117751538754\n",
      "epoch: 34867 loss: 0.0139892203733325\n",
      "epoch: 35071 loss: 0.013758662156760693\n",
      "epoch: 35130 loss: 0.013670407235622406\n",
      "epoch: 35146 loss: 0.013302897103130817\n",
      "epoch: 35217 loss: 0.012920367531478405\n",
      "epoch: 35331 loss: 0.01193638052791357\n",
      "epoch: 35601 loss: 0.01169486902654171\n",
      "epoch: 35650 loss: 0.011371919885277748\n",
      "epoch: 36026 loss: 0.011274184100329876\n",
      "epoch: 36157 loss: 0.010834446176886559\n",
      "epoch: 36397 loss: 0.009920037351548672\n",
      "epoch: 39942 loss: 0.009890194050967693\n",
      "epoch: 40057 loss: 0.009618145413696766\n",
      "epoch: 40187 loss: 0.009499495849013329\n",
      "epoch: 40321 loss: 0.009319907054305077\n",
      "epoch: 40610 loss: 0.008839759044349194\n",
      "epoch: 40889 loss: 0.008748512715101242\n",
      "epoch: 40949 loss: 0.008716107346117496\n",
      "epoch: 41094 loss: 0.008627915754914284\n",
      "epoch: 41112 loss: 0.008616477251052856\n",
      "epoch: 41127 loss: 0.008136711083352566\n",
      "epoch: 41561 loss: 0.0075621213763952255\n",
      "epoch: 41959 loss: 0.007440986577421427\n",
      "epoch: 41989 loss: 0.007368938531726599\n",
      "epoch: 42014 loss: 0.007280254270881414\n",
      "epoch: 42024 loss: 0.007139625493437052\n",
      "epoch: 49683 loss: 0.007004166953265667\n",
      "epoch: 49854 loss: 0.006849045865237713\n",
      "epoch: 49886 loss: 0.006700546946376562\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "bestLoss = 10000\n",
    "bestModel = None\n",
    "\n",
    "for epoch in range(50000):\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    captions_in = torch.tensor(captions[:, :-1], device=device).long()\n",
    "    captions_ou = torch.tensor(captions[:, 1:], device=device).long()\n",
    "    output = policyNetwork(features, captions_in)\n",
    "    \n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        caplen = np.nonzero(captions[i] == 2)[0][0] + 1\n",
    "        loss += (caplen/batch_size)*criterion(output[i][:caplen], captions_ou[i][:caplen])\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        bestModel = policyNetwork\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "torch.save(bestModel.state_dict(), \"policyNetwork.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardNetworkRNN(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(RewardNetworkRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.hidden_cell = torch.zeros(1, 1, self.hidden_dim).to(device)\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        self.gru = nn.GRU(wordvec_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        output, self.hidden_cell = self.gru(input_captions.view(len(input_captions) ,1, -1), self.hidden_cell)\n",
    "        return output\n",
    "    \n",
    "class RewardNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx):\n",
    "        super(RewardNetwork, self).__init__()\n",
    "        self.rewrnn = RewardNetworkRNN(word_to_idx)\n",
    "        self.visual_embed = nn.Linear(512, 512)\n",
    "        self.semantic_embed = nn.Linear(512, 512)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        for t in range(captions.shape[1]):\n",
    "            rrnn = self.rewrnn(captions[:, t])\n",
    "        rrnn = rrnn.squeeze(0).squeeze(1)\n",
    "        se = self.semantic_embed(rrnn)\n",
    "        ve = self.visual_embed(features)\n",
    "        return ve, se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardNetwork = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "optimizer = optim.Adam(rewardNetwork.parameters(), lr=0.001)\n",
    "\n",
    "# https://cs230-stanford.github.io/pytorch-nlp.html#writing-a-custom-loss-function\n",
    "def VisualSemanticEmbeddingLoss(visuals, semantics):\n",
    "    beta = 0.002\n",
    "    N, D = visuals.shape\n",
    "    \n",
    "    visloss = torch.mm(visuals, semantics.t())\n",
    "    visloss = visloss - torch.diag(visloss).unsqueeze(1)\n",
    "    visloss = visloss + (beta/N)*(torch.ones((N, N)).to(device) - torch.eye(N).to(device))\n",
    "    visloss = F.relu(visloss)\n",
    "    visloss = torch.sum(visloss)/N\n",
    "    \n",
    "    semloss = torch.mm(semantics, visuals.t())\n",
    "    semloss = semloss - torch.diag(semloss).unsqueeze(1)\n",
    "    semloss = semloss + (beta/N)*(torch.ones((N, N)).to(device) - torch.eye(N).to(device))\n",
    "    semloss = F.relu(semloss)\n",
    "    semloss = torch.sum(semloss)/N\n",
    "    \n",
    "    return visloss + semloss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = load_coco_data(max_train=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 153.8406219482422\n",
      "epoch: 1 loss: 145.78004455566406\n",
      "epoch: 2 loss: 144.26637268066406\n",
      "epoch: 5 loss: 127.2883529663086\n",
      "epoch: 17 loss: 126.70248413085938\n",
      "epoch: 21 loss: 122.92036437988281\n",
      "epoch: 45 loss: 117.26567077636719\n",
      "epoch: 48 loss: 109.99120330810547\n",
      "epoch: 56 loss: 105.07620239257812\n",
      "epoch: 183 loss: 104.64308166503906\n",
      "epoch: 198 loss: 96.82013702392578\n",
      "epoch: 209 loss: 94.6942138671875\n",
      "epoch: 275 loss: 93.03327178955078\n",
      "epoch: 286 loss: 86.01957702636719\n",
      "epoch: 345 loss: 83.19971466064453\n",
      "epoch: 402 loss: 82.8239974975586\n",
      "epoch: 447 loss: 80.05474853515625\n",
      "epoch: 450 loss: 66.36756134033203\n",
      "epoch: 589 loss: 53.008888244628906\n",
      "epoch: 825 loss: 52.94926452636719\n",
      "epoch: 964 loss: 52.840293884277344\n",
      "epoch: 965 loss: 52.49498748779297\n",
      "epoch: 974 loss: 51.35733413696289\n",
      "epoch: 983 loss: 49.916099548339844\n",
      "epoch: 984 loss: 48.341064453125\n",
      "epoch: 1031 loss: 46.65217590332031\n",
      "epoch: 1099 loss: 44.16111373901367\n",
      "epoch: 1141 loss: 41.64582061767578\n",
      "epoch: 1358 loss: 39.973663330078125\n",
      "epoch: 1380 loss: 39.9705924987793\n",
      "epoch: 1407 loss: 35.39472961425781\n",
      "epoch: 1457 loss: 31.444091796875\n",
      "epoch: 1646 loss: 31.429134368896484\n",
      "epoch: 1727 loss: 31.133546829223633\n",
      "epoch: 1755 loss: 30.463123321533203\n",
      "epoch: 1759 loss: 26.788545608520508\n",
      "epoch: 1827 loss: 26.042255401611328\n",
      "epoch: 1842 loss: 21.898014068603516\n",
      "epoch: 2187 loss: 21.157331466674805\n",
      "epoch: 2309 loss: 18.28589630126953\n",
      "epoch: 2390 loss: 15.919644355773926\n",
      "epoch: 2624 loss: 15.880849838256836\n",
      "epoch: 2631 loss: 15.8115234375\n",
      "epoch: 2673 loss: 14.645584106445312\n",
      "epoch: 2750 loss: 14.125833511352539\n",
      "epoch: 3006 loss: 11.71585464477539\n",
      "epoch: 3238 loss: 11.29970645904541\n",
      "epoch: 3244 loss: 11.111616134643555\n",
      "epoch: 3426 loss: 10.99402904510498\n",
      "epoch: 3429 loss: 10.969250679016113\n",
      "epoch: 3447 loss: 10.89189624786377\n",
      "epoch: 3544 loss: 10.67943286895752\n",
      "epoch: 3550 loss: 10.490233421325684\n",
      "epoch: 3559 loss: 10.176025390625\n",
      "epoch: 3590 loss: 9.266550064086914\n",
      "epoch: 3666 loss: 9.090851783752441\n",
      "epoch: 3719 loss: 8.848204612731934\n",
      "epoch: 3733 loss: 8.70340347290039\n",
      "epoch: 3737 loss: 8.654035568237305\n",
      "epoch: 3761 loss: 8.474893569946289\n",
      "epoch: 3769 loss: 8.241154670715332\n",
      "epoch: 3944 loss: 8.151841163635254\n",
      "epoch: 3980 loss: 7.570710182189941\n",
      "epoch: 4069 loss: 7.558771133422852\n",
      "epoch: 4177 loss: 6.95650577545166\n",
      "epoch: 4196 loss: 6.181880474090576\n",
      "epoch: 4202 loss: 5.635531902313232\n",
      "epoch: 4623 loss: 5.336463451385498\n",
      "epoch: 4853 loss: 5.308542251586914\n",
      "epoch: 4901 loss: 4.913893222808838\n",
      "epoch: 4995 loss: 4.310166835784912\n",
      "epoch: 5045 loss: 4.2184271812438965\n",
      "epoch: 5060 loss: 3.9159979820251465\n",
      "epoch: 5089 loss: 3.868506669998169\n",
      "epoch: 5298 loss: 3.447890281677246\n",
      "epoch: 5555 loss: 3.4005496501922607\n",
      "epoch: 5585 loss: 3.276308536529541\n",
      "epoch: 5679 loss: 3.0235280990600586\n",
      "epoch: 5724 loss: 3.013934850692749\n",
      "epoch: 5751 loss: 2.8977320194244385\n",
      "epoch: 5876 loss: 2.62048602104187\n",
      "epoch: 6221 loss: 2.5130882263183594\n",
      "epoch: 6296 loss: 2.1700332164764404\n",
      "epoch: 6322 loss: 2.0207254886627197\n",
      "epoch: 6832 loss: 1.798125982284546\n",
      "epoch: 7072 loss: 1.671970248222351\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "bestLoss = 10000\n",
    "bestModel = None\n",
    "\n",
    "for epoch in range(50000):\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float()\n",
    "    captions = torch.tensor(captions, device=device).long()\n",
    "    ve, se = rewardNetwork(features, captions)\n",
    "    loss = VisualSemanticEmbeddingLoss(ve, se)\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        bestModel = rewardNetwork\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    rewardNetwork.rewrnn.hidden_cell.detach_()\n",
    "    \n",
    "torch.save(bestModel.state_dict(), \"rewardNetwork.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetworkRNN(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(ValueNetworkRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.hidden_cell = (torch.zeros(1, 1, self.hidden_dim).to(device), torch.zeros(1, 1, self.hidden_dim).to(device))\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        self.lstm = nn.LSTM(wordvec_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        output, self.hidden_cell = self.lstm(input_captions.view(len(input_captions) ,1, -1), self.hidden_cell)\n",
    "        return output\n",
    "    \n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.valrnn = ValueNetworkRNN(word_to_idx)\n",
    "        self.linear1 = nn.Linear(1024, 512)\n",
    "        self.linear2 = nn.Linear(512, 1)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        for t in range(captions.shape[1]):\n",
    "            vrnn = self.valrnn(captions[:, t])\n",
    "        vrnn = vrnn.squeeze(0).squeeze(1)\n",
    "        state = torch.cat((features, vrnn), dim=1)\n",
    "        output = self.linear1(state)\n",
    "        output = self.linear2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueNetwork = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(valueNetwork.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = load_coco_data(max_train=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 1.2763668298721313\n",
      "epoch: 1 loss: 0.8313655257225037\n",
      "epoch: 2 loss: 0.6140240430831909\n",
      "epoch: 3 loss: 0.3055354952812195\n",
      "epoch: 10 loss: 0.23417949676513672\n",
      "epoch: 12 loss: 0.18680809438228607\n",
      "epoch: 13 loss: 0.18397590517997742\n",
      "epoch: 19 loss: 0.15089599788188934\n",
      "epoch: 21 loss: 0.10812809318304062\n",
      "epoch: 24 loss: 0.06933774799108505\n",
      "epoch: 25 loss: 0.047363847494125366\n",
      "epoch: 33 loss: 0.04467802867293358\n",
      "epoch: 35 loss: 0.03266926854848862\n",
      "epoch: 47 loss: 0.032051242887973785\n",
      "epoch: 49 loss: 0.02492024190723896\n",
      "epoch: 50 loss: 0.020599478855729103\n",
      "epoch: 57 loss: 0.01787969097495079\n",
      "epoch: 58 loss: 0.015650803223252296\n",
      "epoch: 60 loss: 0.012111949734389782\n",
      "epoch: 62 loss: 0.011107435449957848\n",
      "epoch: 71 loss: 0.010738027282059193\n",
      "epoch: 74 loss: 0.010489401407539845\n",
      "epoch: 76 loss: 0.00931495614349842\n",
      "epoch: 78 loss: 0.007649941835552454\n",
      "epoch: 80 loss: 0.006788589991629124\n",
      "epoch: 83 loss: 0.0060847424902021885\n",
      "epoch: 89 loss: 0.005905819125473499\n",
      "epoch: 92 loss: 0.005778406746685505\n",
      "epoch: 93 loss: 0.0040917894802987576\n",
      "epoch: 94 loss: 0.0037770855706185102\n",
      "epoch: 95 loss: 0.0030270135030150414\n",
      "epoch: 102 loss: 0.002368984278291464\n",
      "epoch: 114 loss: 0.0014839795185253024\n",
      "epoch: 122 loss: 0.001309338491410017\n",
      "epoch: 129 loss: 0.000961720768827945\n",
      "epoch: 140 loss: 0.0007052552537061274\n",
      "epoch: 147 loss: 0.0005657201982103288\n",
      "epoch: 150 loss: 0.0005009036976844072\n",
      "epoch: 166 loss: 0.0004672683135140687\n",
      "epoch: 167 loss: 0.0002980217686854303\n",
      "epoch: 175 loss: 0.00028638693038374186\n",
      "epoch: 179 loss: 0.00025119015481323004\n",
      "epoch: 184 loss: 0.00023960167891345918\n",
      "epoch: 189 loss: 0.00022319945855997503\n",
      "epoch: 193 loss: 0.00020871676679234952\n",
      "epoch: 194 loss: 0.00018658449698705226\n",
      "epoch: 199 loss: 0.00017820438370108604\n",
      "epoch: 200 loss: 0.00013756730186287314\n",
      "epoch: 207 loss: 0.00012110240641050041\n",
      "epoch: 212 loss: 0.00010865671356441453\n",
      "epoch: 221 loss: 8.916515071177855e-05\n",
      "epoch: 224 loss: 7.355258276220411e-05\n",
      "epoch: 225 loss: 7.13398476364091e-05\n",
      "epoch: 229 loss: 5.800078724860214e-05\n",
      "epoch: 238 loss: 5.1727420213865116e-05\n",
      "epoch: 239 loss: 5.1576011173892766e-05\n",
      "epoch: 242 loss: 5.103463263367303e-05\n",
      "epoch: 244 loss: 4.696159157902002e-05\n",
      "epoch: 246 loss: 4.5620723540196195e-05\n",
      "epoch: 247 loss: 4.457885734154843e-05\n",
      "epoch: 248 loss: 3.4495911677367985e-05\n",
      "epoch: 257 loss: 3.207976988051087e-05\n",
      "epoch: 263 loss: 3.202946754754521e-05\n",
      "epoch: 264 loss: 2.3872855308582075e-05\n",
      "epoch: 267 loss: 2.292292265337892e-05\n",
      "epoch: 269 loss: 1.92243569472339e-05\n",
      "epoch: 332 loss: 1.4128869224805385e-05\n",
      "epoch: 404 loss: 1.3471597412717529e-05\n",
      "epoch: 415 loss: 1.2076070561306551e-05\n",
      "epoch: 424 loss: 1.0800804375321604e-05\n",
      "epoch: 430 loss: 1.0223633580608293e-05\n",
      "epoch: 434 loss: 8.899616659618914e-06\n",
      "epoch: 447 loss: 7.641068805241957e-06\n",
      "epoch: 453 loss: 7.4972490438085515e-06\n",
      "epoch: 454 loss: 6.4539594859525096e-06\n",
      "epoch: 465 loss: 4.596985490934458e-06\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "bestLoss = 10000\n",
    "bestModel = None\n",
    "\n",
    "for epoch in range(50000):\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float()\n",
    "    captions = torch.tensor(captions, device=device).long()\n",
    "    output = valueNetwork(features, captions)\n",
    "    loss = criterion(output, torch.tensor([[1]]*50).float().to(device))\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        bestModel = rewardNetwork\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    valueNetwork.valrnn.hidden_cell[0].detach_()\n",
    "    valueNetwork.valrnn.hidden_cell[1].detach_()\n",
    "\n",
    "torch.save(bestModel.state_dict(), \"valueNetwork.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
