{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning-based Image Captioning with Embedding Reward\n",
    "Pranshu Gupta, Deep Learning @ Georgia Institute of Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Working on:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from utils.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from utils.image_utils import image_from_url\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Working on: \", device)\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "max_seq_len = 17\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MS-COCO data\n",
    "We will use the Microsoft COCO dataset for captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
      "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
      "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
      "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
      "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
      "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
      "idx_to_word <class 'list'> 1004\n",
      "word_to_idx <class 'dict'> 1004\n",
      "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
      "val_urls <class 'numpy.ndarray'> (40504,) <U63\n",
      "train_captions_lens <class 'numpy.ndarray'> (400135,) float64\n",
      "val_captions_lens <class 'numpy.ndarray'> (195954,) float64\n"
     ]
    }
   ],
   "source": [
    "# Load COCO data from disk; this returns a dictionary\n",
    "# We'll work with dimensionality-reduced features for this notebook, but feel\n",
    "# free to experiment with the original features by changing the flag below.\n",
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "data[\"train_captions_lens\"] = np.zeros(data[\"train_captions\"].shape[0])\n",
    "data[\"val_captions_lens\"] = np.zeros(data[\"val_captions\"].shape[0])\n",
    "for i in range(data[\"train_captions\"].shape[0]):\n",
    "    data[\"train_captions_lens\"][i] = np.nonzero(data[\"train_captions\"][i] == 2)[0][0] + 1\n",
    "for i in range(data[\"val_captions\"].shape[0]):\n",
    "    data[\"val_captions_lens\"][i] = np.nonzero(data[\"val_captions\"][i] == 2)[0][0] + 1\n",
    "\n",
    "\n",
    "# Print out all the keys and values from the data dictionary\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = load_coco_data(max_train=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        \n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        \n",
    "        self.cnn2linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(wordvec_dim, hidden_dim, batch_first=True)\n",
    "        self.linear2vocab = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        hidden_init = self.cnn2linear(features)\n",
    "        cell_init = torch.zeros_like(hidden_init)\n",
    "        output, _ = self.lstm(input_captions, (hidden_init, cell_init))\n",
    "        output = self.linear2vocab(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardNetworkRNN(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(RewardNetworkRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.hidden_cell = torch.zeros(1, 1, self.hidden_dim).to(device)\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        self.gru = nn.GRU(wordvec_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        output, self.hidden_cell = self.gru(input_captions.view(len(input_captions) ,1, -1), self.hidden_cell)\n",
    "        return output\n",
    "    \n",
    "class RewardNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx):\n",
    "        super(RewardNetwork, self).__init__()\n",
    "        self.rewrnn = RewardNetworkRNN(word_to_idx)\n",
    "        self.visual_embed = nn.Linear(512, 512)\n",
    "        self.semantic_embed = nn.Linear(512, 512)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        for t in range(captions.shape[1]):\n",
    "            rrnn = self.rewrnn(captions[:, t])\n",
    "        rrnn = rrnn.squeeze(0).squeeze(1)\n",
    "        se = self.semantic_embed(rrnn)\n",
    "        ve = self.visual_embed(features)\n",
    "        return ve, se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetworkRNN(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(ValueNetworkRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.hidden_cell = (torch.zeros(1, 1, self.hidden_dim).to(device), torch.zeros(1, 1, self.hidden_dim).to(device))\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        self.lstm = nn.LSTM(wordvec_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        output, self.hidden_cell = self.lstm(input_captions.view(len(input_captions) ,1, -1), self.hidden_cell)\n",
    "        return output\n",
    "    \n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.valrnn = ValueNetworkRNN(word_to_idx)\n",
    "        self.linear1 = nn.Linear(1024, 512)\n",
    "        self.linear2 = nn.Linear(512, 1)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        for t in range(captions.shape[1]):\n",
    "            vrnn = self.valrnn(captions[:, t])\n",
    "        vrnn = vrnn.squeeze(0).squeeze(1)\n",
    "        state = torch.cat((features, vrnn), dim=1)\n",
    "        output = self.linear1(state)\n",
    "        output = self.linear2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValueNetwork(\n",
       "  (valrnn): ValueNetworkRNN(\n",
       "    (caption_embedding): Embedding(1004, 512)\n",
       "    (lstm): LSTM(512, 512)\n",
       "  )\n",
       "  (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "policyNet.load_state_dict(torch.load('policyNetwork.pt'))\n",
    "policyNet.train(mode=False)\n",
    "\n",
    "valueNet = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "valueNet.load_state_dict(torch.load('valueNetwork.pt'))\n",
    "valueNet.train(mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Caption Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCaptions(features, captions, model):\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    gen_caps = torch.tensor(captions[:, 0:1], device=device).long()\n",
    "    for t in range(max_seq_len-1):\n",
    "        output = model(features, gen_caps)\n",
    "        gen_caps = torch.cat((gen_caps, output[:, -1:, :].argmax(axis=2)), axis=1)\n",
    "    return gen_caps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search Caption Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCaptionsWithBeamSearch(features, captions, model, beamSize=5):\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    gen_caps = torch.tensor(captions[:, 0:1], device=device).long()\n",
    "    candidates = [(gen_caps, 0)]\n",
    "    for t in range(max_seq_len-1):\n",
    "        next_candidates = []\n",
    "        for c in range(len(candidates)):\n",
    "            output = model(features, candidates[c][0])\n",
    "            probs, words = torch.topk(output[:, -1:, :], beamSize)\n",
    "            for i in range(beamSize):\n",
    "                cap = torch.cat((candidates[c][0], words[:, :, i]), axis=1)\n",
    "                score = candidates[c][1] - torch.log(probs[0, 0, i]).item()\n",
    "                next_candidates.append((cap, score))\n",
    "        ordered_candidates = sorted(next_candidates, key=lambda tup:tup[1])\n",
    "        candidates = ordered_candidates[:beamSize]\n",
    "    return candidates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lookahead Inference with Policy and Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCaptionsWithBeamSearchValueScoring(features, captions, model, beamSize=5):\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    gen_caps = torch.tensor(captions[:, 0:1], device=device).long()\n",
    "    candidates = [(gen_caps, 0)]\n",
    "    for t in range(max_seq_len-1):\n",
    "        next_candidates = []\n",
    "        for c in range(len(candidates)):\n",
    "            output = model(features, candidates[c][0])\n",
    "            probs, words = torch.topk(output[:, -1:, :], beamSize)\n",
    "            for i in range(beamSize):\n",
    "                cap = torch.cat((candidates[c][0], words[:, :, i]), axis=1)\n",
    "                value = valueNet(features.squeeze(0), cap).detach()\n",
    "                score = candidates[c][1] - 0.6*value.item() -0.4*torch.log(probs[0, 0, i]).item()\n",
    "                next_candidates.append((cap, score))\n",
    "        ordered_candidates = sorted(next_candidates, key=lambda tup:tup[1])\n",
    "        candidates = ordered_candidates[:beamSize]\n",
    "    return candidates   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    max_seq_len = 17\n",
    "    captions, features, urls = sample_coco_minibatch(small_data, batch_size=1000, split='val')\n",
    "    for i in range(1000):\n",
    "        gen_caps = []\n",
    "        gen_caps.append(GenerateCaptions(features[i:i+1], captions[i:i+1], policyNet)[0])\n",
    "        gen_caps.append(GenerateCaptionsWithBeamSearch(features[i:i+1], captions[i:i+1], policyNet)[0][0][0])\n",
    "        gen_caps.append(GenerateCaptionsWithBeamSearchValueScoring(features[i:i+1], captions[i:i+1], policyNet)[0][0][0])\n",
    "        decoded_tru_caps = decode_captions(captions[i], data[\"idx_to_word\"])\n",
    "\n",
    "        f = open(\"truth3.txt\", \"a\")\n",
    "        f.write(decoded_tru_caps + \"\\n\")\n",
    "        f.close()\n",
    "        \n",
    "        decoded_gen_caps = decode_captions(gen_caps[0], data[\"idx_to_word\"])\n",
    "        f = open(\"greedy3.txt\", \"a\")\n",
    "        f.write(decoded_gen_caps + \"\\n\")\n",
    "        f.close()\n",
    "        \n",
    "        decoded_gen_caps = decode_captions(gen_caps[1], data[\"idx_to_word\"])\n",
    "        f = open(\"beam3.txt\", \"a\")\n",
    "        f.write(decoded_gen_caps + \"\\n\")\n",
    "        f.close()\n",
    "        \n",
    "        decoded_gen_caps = decode_captions(gen_caps[2], data[\"idx_to_word\"])\n",
    "        f = open(\"policyvalue3.txt\", \"a\")\n",
    "        f.write(decoded_gen_caps + \"\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLEU_score(gt_caption, sample_caption, w):\n",
    "    \"\"\"\n",
    "    gt_caption: string, ground-truth caption\n",
    "    sample_caption: string, your model's predicted caption\n",
    "    Returns unigram BLEU score.\n",
    "    \"\"\"\n",
    "    reference = [x for x in gt_caption.split(' ') \n",
    "                 if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "    hypothesis = [x for x in sample_caption.split(' ') \n",
    "                  if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = [w])\n",
    "    return BLEUscore\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"\n",
    "    model: CaptioningRNN model\n",
    "    Prints unigram BLEU score averaged over 1000 training and val examples.\n",
    "    \"\"\"\n",
    "    BLEUscores = {}\n",
    "    for split in ['train', 'val']:\n",
    "        minibatch = sample_coco_minibatch(data, split=split, batch_size=1000)\n",
    "        gt_captions, features, urls = minibatch\n",
    "        gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n",
    "\n",
    "        sample_captions = model.sample(features)\n",
    "        sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n",
    "\n",
    "        total_score = 0.0\n",
    "        for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n",
    "            total_score += BLEU_score(gt_caption, sample_caption)\n",
    "\n",
    "        BLEUscores[split] = total_score / len(sample_captions)\n",
    "\n",
    "    for split in BLEUscores:\n",
    "        print('Average BLEU score for %s: %f' % (split, BLEUscores[split]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps0 = []\n",
    "caps1 = []\n",
    "caps2 = []\n",
    "caps3 = []\n",
    "f = open(\"truth2.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps0.append(x)\n",
    "f = open(\"greedy2.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps1.append(x)\n",
    "f = open(\"beam2.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps2.append(x)\n",
    "f = open(\"policyvalue2.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps3.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy BLEU-1 : 0.3374543171912208\n",
      "Beam BLEU-1 : 0.29998119046207933\n",
      "Agent BLEU-1 : 0.30057253835441977\n",
      "\n",
      "Greedy BLEU-2 : 0.18381039209700356\n",
      "Beam BLEU-2 : 0.13227059725207552\n",
      "Agent BLEU-2 : 0.1331405488671185\n",
      "\n",
      "Greedy BLEU-3 : 0.12767973218661097\n",
      "Beam BLEU-3 : 0.0724795981070803\n",
      "Agent BLEU-3 : 0.07345311185936992\n",
      "\n",
      "Greedy BLEU-4 : 0.10370808190929426\n",
      "Beam BLEU-4 : 0.04722721512979743\n",
      "Agent BLEU-4 : 0.04818825165316483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b1, b2, b3 = 0, 0, 0\n",
    "for w in range(1, 5):\n",
    "    for i in range(len(caps0)):\n",
    "        b1 += BLEU_score(caps0[i], caps1[i], w)\n",
    "        b2 += BLEU_score(caps0[i], caps2[i], w)\n",
    "        b3 += BLEU_score(caps0[i], caps3[i], w)\n",
    "    b1 /= len(caps0)\n",
    "    b2 /= len(caps0)\n",
    "    b3 /= len(caps0)\n",
    "    print(\"Greedy BLEU-\" + str(w), \":\", b1)\n",
    "    print(\"Beam BLEU-\" + str(w), \":\", b2)\n",
    "    print(\"Agent BLEU-\" + str(w), \":\", b3)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of references is 1000\n",
      "{'testlen': 10255, 'reflen': 9324, 'guess': [10255, 9255, 8255, 7255], 'correct': [3285, 1002, 451, 270]}\n",
      "ratio: 1.0998498498497318\n",
      "{'Bleu_1': 0.32033154558748705, 'Bleu_2': 0.18622822496901026, 'Bleu_3': 0.12374191249218547, 'Bleu_4': 0.09163665014876797, 'METEOR': 0.14102364160475023, 'ROUGE_L': 0.30709802012803344, 'CIDEr': 0.8480455366206312}\n"
     ]
    }
   ],
   "source": [
    "ref, hypo = metrics.load_textfiles(caps0, caps3)\n",
    "print(metrics.score(ref, hypo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = False\n",
    "\n",
    "policyNetwork = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(policyNetwork.parameters(), lr=0.0001)\n",
    "\n",
    "if pretrained:\n",
    "    policyNetwork.load_state_dict(torch.load('models/policyNetwork.pt'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 256846 loss: 0.2919604778289795\n",
      "epoch: 271081 loss: 0.2918578088283539\n",
      "epoch: 276987 loss: 0.2405262291431427\n",
      "epoch: 324988 loss: 0.2366735190153122\n",
      "epoch: 339368 loss: 0.20999650657176971\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "bestLoss = 0.3\n",
    "#0.006700546946376562\n",
    "\n",
    "for epoch in range(250000, 350000):\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    captions_in = torch.tensor(captions[:, :-1], device=device).long()\n",
    "    captions_ou = torch.tensor(captions[:, 1:], device=device).long()\n",
    "    output = policyNetwork(features, captions_in)\n",
    "    \n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        caplen = np.nonzero(captions[i] == 2)[0][0] + 1\n",
    "        loss += (caplen/batch_size)*criterion(output[i][:caplen], captions_ou[i][:caplen])\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        torch.save(policyNetwork.state_dict(), \"policyNetwork.pt\")\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Reward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardNetwork = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "optimizer = optim.Adam(rewardNetwork.parameters(), lr=0.001)\n",
    "\n",
    "# https://cs230-stanford.github.io/pytorch-nlp.html#writing-a-custom-loss-function\n",
    "def VisualSemanticEmbeddingLoss(visuals, semantics):\n",
    "    beta = 0.2\n",
    "    N, D = visuals.shape\n",
    "    \n",
    "    visloss = torch.mm(visuals, semantics.t())\n",
    "    visloss = visloss - torch.diag(visloss).unsqueeze(1)\n",
    "    visloss = visloss + (beta/N)*(torch.ones((N, N)).to(device) - torch.eye(N).to(device))\n",
    "    visloss = F.relu(visloss)\n",
    "    visloss = torch.sum(visloss)/N\n",
    "    \n",
    "    semloss = torch.mm(semantics, visuals.t())\n",
    "    semloss = semloss - torch.diag(semloss).unsqueeze(1)\n",
    "    semloss = semloss + (beta/N)*(torch.ones((N, N)).to(device) - torch.eye(N).to(device))\n",
    "    semloss = F.relu(semloss)\n",
    "    semloss = torch.sum(semloss)/N\n",
    "    \n",
    "    return visloss + semloss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 285.8268737792969\n",
      "epoch: 78 loss: 279.0987243652344\n",
      "epoch: 109 loss: 268.0894775390625\n",
      "epoch: 140 loss: 258.7628173828125\n",
      "epoch: 146 loss: 249.2835235595703\n",
      "epoch: 184 loss: 242.68533325195312\n",
      "epoch: 198 loss: 234.36572265625\n",
      "epoch: 271 loss: 219.52809143066406\n",
      "epoch: 276 loss: 201.01954650878906\n",
      "epoch: 499 loss: 190.47842407226562\n",
      "epoch: 521 loss: 184.58734130859375\n",
      "epoch: 608 loss: 181.97393798828125\n",
      "epoch: 614 loss: 161.46424865722656\n",
      "epoch: 663 loss: 159.47470092773438\n",
      "epoch: 702 loss: 128.3231964111328\n",
      "epoch: 847 loss: 103.04296112060547\n",
      "epoch: 1151 loss: 100.62371063232422\n",
      "epoch: 1182 loss: 99.61503601074219\n",
      "epoch: 1204 loss: 96.04625701904297\n",
      "epoch: 1273 loss: 95.47994995117188\n",
      "epoch: 1316 loss: 90.17144012451172\n",
      "epoch: 1413 loss: 80.14276885986328\n",
      "epoch: 1482 loss: 80.06565856933594\n",
      "epoch: 1505 loss: 75.33641815185547\n",
      "epoch: 1600 loss: 73.59349060058594\n",
      "epoch: 1637 loss: 68.0627670288086\n",
      "epoch: 1653 loss: 67.85617065429688\n",
      "epoch: 1690 loss: 65.30216217041016\n",
      "epoch: 1733 loss: 64.63203430175781\n",
      "epoch: 1834 loss: 64.56878662109375\n",
      "epoch: 1857 loss: 60.83083724975586\n",
      "epoch: 1981 loss: 60.30710220336914\n",
      "epoch: 1986 loss: 49.458648681640625\n",
      "epoch: 2196 loss: 45.71683883666992\n",
      "epoch: 2248 loss: 43.26191711425781\n",
      "epoch: 2312 loss: 41.79459762573242\n",
      "epoch: 2330 loss: 40.281280517578125\n",
      "epoch: 2495 loss: 39.746768951416016\n",
      "epoch: 2497 loss: 35.74808120727539\n",
      "epoch: 2512 loss: 34.61565399169922\n",
      "epoch: 2568 loss: 33.60615539550781\n",
      "epoch: 2626 loss: 33.28276824951172\n",
      "epoch: 2651 loss: 33.172630310058594\n",
      "epoch: 2671 loss: 33.140159606933594\n",
      "epoch: 2685 loss: 33.01344299316406\n",
      "epoch: 2741 loss: 32.708580017089844\n",
      "epoch: 2743 loss: 31.37051010131836\n",
      "epoch: 2753 loss: 30.7443790435791\n",
      "epoch: 2793 loss: 29.888378143310547\n",
      "epoch: 2822 loss: 26.44160270690918\n",
      "epoch: 2911 loss: 26.382713317871094\n",
      "epoch: 3007 loss: 24.723159790039062\n",
      "epoch: 3011 loss: 24.342050552368164\n",
      "epoch: 3026 loss: 23.810487747192383\n",
      "epoch: 3041 loss: 23.61092185974121\n",
      "epoch: 3265 loss: 23.290103912353516\n",
      "epoch: 3278 loss: 19.378631591796875\n",
      "epoch: 3440 loss: 18.06199836730957\n",
      "epoch: 3572 loss: 17.654245376586914\n",
      "epoch: 3575 loss: 16.850923538208008\n",
      "epoch: 3576 loss: 16.7466983795166\n",
      "epoch: 3596 loss: 14.580103874206543\n",
      "epoch: 3692 loss: 14.511017799377441\n",
      "epoch: 3876 loss: 14.472853660583496\n",
      "epoch: 3978 loss: 13.664694786071777\n",
      "epoch: 4109 loss: 12.842731475830078\n",
      "epoch: 4123 loss: 12.740828514099121\n",
      "epoch: 4130 loss: 12.596888542175293\n",
      "epoch: 4184 loss: 12.536019325256348\n",
      "epoch: 4250 loss: 12.382783889770508\n",
      "epoch: 4273 loss: 11.434013366699219\n",
      "epoch: 4420 loss: 8.472916603088379\n",
      "epoch: 5030 loss: 7.869298458099365\n",
      "epoch: 5188 loss: 7.837207794189453\n",
      "epoch: 5252 loss: 7.104841232299805\n",
      "epoch: 5337 loss: 7.026317596435547\n",
      "epoch: 5415 loss: 6.969333648681641\n",
      "epoch: 5422 loss: 6.925944805145264\n",
      "epoch: 5431 loss: 6.212750434875488\n",
      "epoch: 5511 loss: 6.042999267578125\n",
      "epoch: 5520 loss: 5.607885360717773\n",
      "epoch: 5539 loss: 5.481804370880127\n",
      "epoch: 5892 loss: 4.893679618835449\n",
      "epoch: 5903 loss: 4.795295715332031\n",
      "epoch: 5987 loss: 4.580246448516846\n",
      "epoch: 6004 loss: 4.018287181854248\n",
      "epoch: 6353 loss: 3.917495012283325\n",
      "epoch: 6361 loss: 3.7427287101745605\n",
      "epoch: 6386 loss: 3.6220428943634033\n",
      "epoch: 6389 loss: 3.51171875\n",
      "epoch: 6409 loss: 3.4881749153137207\n",
      "epoch: 6411 loss: 3.395183563232422\n",
      "epoch: 6527 loss: 3.3584117889404297\n",
      "epoch: 6659 loss: 3.28605318069458\n",
      "epoch: 7065 loss: 2.9053308963775635\n",
      "epoch: 7154 loss: 2.4895424842834473\n",
      "epoch: 7312 loss: 2.4348840713500977\n",
      "epoch: 7469 loss: 2.3489034175872803\n",
      "epoch: 7583 loss: 2.297377586364746\n",
      "epoch: 7781 loss: 2.242737054824829\n",
      "epoch: 7783 loss: 2.089797258377075\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-c307acabf195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_coco_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewardNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "bestLoss = 10000\n",
    "\n",
    "for epoch in range(50000):\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float()\n",
    "    captions = torch.tensor(captions, device=device).long()\n",
    "    ve, se = rewardNetwork(features, captions)\n",
    "    loss = VisualSemanticEmbeddingLoss(ve, se)\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        torch.save(rewardNetwork.state_dict(), \"rewardNetwork.pt\")\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    rewardNetwork.rewrnn.hidden_cell.detach_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRewards(features, captions, model):\n",
    "    visEmbeds, semEmbeds = model(features, captions)\n",
    "    visEmbeds = F.normalize(visEmbeds, p=2, dim=1) \n",
    "    semEmbeds = F.normalize(semEmbeds, p=2, dim=1) \n",
    "    rewards = torch.sum(visEmbeds*semEmbeds, axis=1).unsqueeze(1)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RewardNetwork(\n",
      "  (rewrnn): RewardNetworkRNN(\n",
      "    (caption_embedding): Embedding(1004, 512)\n",
      "    (gru): GRU(512, 512)\n",
      "  )\n",
      "  (visual_embed): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (semantic_embed): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "PolicyNetwork(\n",
      "  (caption_embedding): Embedding(1004, 512)\n",
      "  (cnn2linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (lstm): LSTM(512, 512, batch_first=True)\n",
      "  (linear2vocab): Linear(in_features=512, out_features=1004, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ValueNetwork(\n",
       "  (valrnn): ValueNetworkRNN(\n",
       "    (caption_embedding): Embedding(1004, 512)\n",
       "    (lstm): LSTM(512, 512)\n",
       "  )\n",
       "  (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewardNet = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "rewardNet.load_state_dict(torch.load('rewardNetwork.pt'))\n",
    "for param in rewardNet.parameters():\n",
    "    param.require_grad = False\n",
    "print(rewardNet)\n",
    "\n",
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "policyNet.load_state_dict(torch.load('policyNetwork.pt'))\n",
    "for param in policyNet.parameters():\n",
    "    param.require_grad = False\n",
    "print(policyNet)\n",
    "\n",
    "valueNetwork = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(valueNetwork.parameters(), lr=0.0001)\n",
    "valueNetwork.train(mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patcha/anaconda3/envs/pranshu/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.4791761636734009\n",
      "epoch: 1 loss: 0.3826758861541748\n",
      "epoch: 2 loss: 0.30105552077293396\n",
      "epoch: 3 loss: 0.2344961166381836\n",
      "epoch: 5 loss: 0.21635407209396362\n",
      "epoch: 6 loss: 0.2095080465078354\n",
      "epoch: 8 loss: 0.17506301403045654\n",
      "epoch: 9 loss: 0.16135434806346893\n",
      "epoch: 11 loss: 0.10012535750865936\n",
      "epoch: 13 loss: 0.07867062091827393\n",
      "epoch: 21 loss: 0.07740631699562073\n",
      "epoch: 22 loss: 0.07632629573345184\n",
      "epoch: 23 loss: 0.05278187245130539\n",
      "epoch: 29 loss: 0.0399894043803215\n",
      "epoch: 34 loss: 0.03727782517671585\n",
      "epoch: 36 loss: 0.033582452684640884\n",
      "epoch: 39 loss: 0.03300255537033081\n",
      "epoch: 44 loss: 0.027580423280596733\n",
      "epoch: 45 loss: 0.025160321965813637\n",
      "epoch: 48 loss: 0.019739700481295586\n",
      "epoch: 50 loss: 0.01948259025812149\n",
      "epoch: 52 loss: 0.013901742175221443\n",
      "epoch: 57 loss: 0.013447004370391369\n",
      "epoch: 59 loss: 0.012788197956979275\n",
      "epoch: 69 loss: 0.010633053258061409\n",
      "epoch: 71 loss: 0.008541139774024487\n",
      "epoch: 76 loss: 0.007344955112785101\n",
      "epoch: 87 loss: 0.007163235917687416\n",
      "epoch: 88 loss: 0.007104307413101196\n",
      "epoch: 92 loss: 0.006274300627410412\n",
      "epoch: 98 loss: 0.00594090111553669\n",
      "epoch: 99 loss: 0.005542732309550047\n",
      "epoch: 106 loss: 0.005477550905197859\n",
      "epoch: 108 loss: 0.00512406462803483\n",
      "epoch: 114 loss: 0.004676204640418291\n",
      "epoch: 119 loss: 0.004628470633178949\n",
      "epoch: 125 loss: 0.003806095104664564\n",
      "epoch: 128 loss: 0.0035750423558056355\n",
      "epoch: 135 loss: 0.0032683389727026224\n",
      "epoch: 152 loss: 0.0032380204647779465\n",
      "epoch: 188 loss: 0.0030650447588413954\n",
      "epoch: 195 loss: 0.0030461729038506746\n",
      "epoch: 216 loss: 0.0028899353928864002\n",
      "epoch: 289 loss: 0.0026609667111188173\n",
      "epoch: 360 loss: 0.0024584231432527304\n",
      "epoch: 3813 loss: 0.002223054878413677\n",
      "epoch: 5650 loss: 0.0020983361173421144\n",
      "epoch: 5758 loss: 0.002071082592010498\n",
      "epoch: 6157 loss: 0.0016574194887652993\n",
      "epoch: 6596 loss: 0.0015082096215337515\n",
      "epoch: 7947 loss: 0.0013867637608200312\n",
      "epoch: 9072 loss: 0.0013866648077964783\n",
      "epoch: 9700 loss: 0.001355444430373609\n",
      "epoch: 10034 loss: 0.0013036648742854595\n",
      "epoch: 11070 loss: 0.0013032826827839017\n",
      "epoch: 11803 loss: 0.0011801589280366898\n",
      "epoch: 12673 loss: 0.0010152908507734537\n",
      "epoch: 17053 loss: 0.0009397168760187924\n",
      "epoch: 17345 loss: 0.0009105623466894031\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-1553aa94b6a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Compute the value of a random state in the generation process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#     print(features.shape, captions[:, :random.randint(1, 17)].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalueNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Compute the loss for the value and the reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pranshu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-cc0e350a0750>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, captions)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mvrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mvrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvrnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pranshu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-cc0e350a0750>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, captions)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0minput_captions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaption_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_captions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_captions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pranshu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pranshu/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pranshu/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pranshu/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 526\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "bestLoss = 10000\n",
    "max_seq_len = 17\n",
    "\n",
    "for epoch in range(50000):\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float()\n",
    "    \n",
    "    # Generate captions using the policy network\n",
    "    captions = GenerateCaptions(features, captions, policyNet)\n",
    "    \n",
    "    # Compute the reward of the generated caption using reward network\n",
    "    rewards = GetRewards(features, captions, rewardNet)\n",
    "    \n",
    "    # Compute the value of a random state in the generation process\n",
    "#     print(features.shape, captions[:, :random.randint(1, 17)].shape)\n",
    "    values = valueNetwork(features, captions[:, :random.randint(1, 17)])\n",
    "    \n",
    "    # Compute the loss for the value and the reward\n",
    "    loss = criterion(values, rewards)\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        torch.save(valueNetwork.state_dict(), \"valueNetwork.pt\")\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    valueNetwork.valrnn.hidden_cell[0].detach_()\n",
    "    valueNetwork.valrnn.hidden_cell[1].detach_()\n",
    "    rewardNet.rewrnn.hidden_cell.detach_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "Advantage Actor Critic Model for Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvantageActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, valueNet, policyNet):\n",
    "        super(AdvantageActorCriticNetwork, self).__init__()\n",
    "\n",
    "        self.valueNet = valueNet #RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "        self.policyNet = policyNet #PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # Get value from value network\n",
    "        values = self.valueNet(features, captions)\n",
    "        # Get action probabilities from policy network\n",
    "        probs = self.policyNet(features.unsqueeze(0), captions)[:, -1:, :]        \n",
    "        return values, probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardNet = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "valueNet = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "\n",
    "rewardNet.load_state_dict(torch.load('rewardNetwork.pt'))\n",
    "policyNet.load_state_dict(torch.load('policyNetwork.pt'))\n",
    "valueNet.load_state_dict(torch.load('valueNetwork.pt'))\n",
    "\n",
    "a2cNetwork = AdvantageActorCriticNetwork(valueNet, policyNet)\n",
    "optimizer = optim.Adam(a2cNetwork.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.011274048415052675\n",
      "1 : 0.009052214500998157\n",
      "2 : 0.006383603059908865\n",
      "3 : 0.0073232187853773225\n",
      "4 : 0.0034985836075793475\n",
      "5 : 0.0041928653282229805\n",
      "6 : 0.0018523795278451878\n",
      "7 : -0.0031698185374989434\n",
      "8 : 0.007850690340856092\n",
      "9 : 0.014547815381083636\n",
      "10 : -0.00046384327535633953\n",
      "11 : 0.0036360891032381913\n",
      "12 : 0.007600265605142341\n",
      "13 : -0.002000644241570627\n",
      "14 : -0.0028894532122649254\n",
      "15 : 0.014178393029305885\n",
      "16 : -0.002927510830195387\n",
      "17 : -0.0004923985352070305\n",
      "18 : 0.003888461572641971\n",
      "19 : -7.64965608823334e-06\n",
      "20 : 0.008021804855125084\n",
      "21 : 0.006459024113355553\n",
      "22 : -0.007037104693663421\n",
      "23 : 0.005563439957350054\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-feec9e4e943a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGetRewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewardNet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-e7a2df754ed3>\u001b[0m in \u001b[0;36mGetRewards\u001b[0;34m(features, captions, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mGetRewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mvisEmbeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemEmbeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mvisEmbeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisEmbeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msemEmbeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemEmbeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisEmbeds\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msemEmbeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pranshu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-c30aa1131594>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, captions)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mrrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mrrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemantic_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrrnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "curriculum = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "episodes = 50\n",
    "\n",
    "small_data = load_coco_data(max_train=50000)\n",
    "\n",
    "for level in curriculum:\n",
    "    \n",
    "    for epoch in range(1000):        \n",
    "        episodicAvgLoss = 0\n",
    "        \n",
    "        captions, features, _ = sample_coco_minibatch(small_data, batch_size=episodes, split='train')\n",
    "        features = torch.tensor(features, device=device).float()\n",
    "        captions = torch.tensor(captions, device=device).long()\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            log_probs = []\n",
    "            values = []\n",
    "            rewards = []\n",
    "            caplen = np.nonzero(captions[episode] == 2)[0][0] + 1\n",
    "            \n",
    "            if (caplen - level > 1):\n",
    "                captions_in = captions[episode:episode+1, :caplen-level]\n",
    "                features_in = features[episode:episode+1]\n",
    "\n",
    "                for step in range(level):\n",
    "                    value, probs = a2cNetwork(features_in, captions_in)\n",
    "                    probs = F.softmax(probs, dim=2)\n",
    "                    \n",
    "                    dist = probs.cpu().detach().numpy()[0,0]\n",
    "                    action = np.random.choice(probs.shape[-1], p=dist)\n",
    "                    \n",
    "                    gen_cap = torch.from_numpy(np.array([action])).unsqueeze(0).to(device)\n",
    "                    captions_in = torch.cat((captions_in, gen_cap), axis=1)\n",
    "                    \n",
    "                    log_prob = torch.log(probs[0, 0, action])\n",
    "                    \n",
    "                    reward = GetRewards(features_in, captions_in, rewardNet)\n",
    "                    reward = reward.cpu().detach().numpy()[0, 0]\n",
    "                    \n",
    "                    rewards.append(reward)\n",
    "                    values.append(value)\n",
    "                    log_probs.append(log_prob)\n",
    "                    \n",
    "            values = torch.FloatTensor(values).to(device)\n",
    "            rewards = torch.FloatTensor(rewards).to(device)\n",
    "            log_probs = torch.stack(log_probs).to(device)\n",
    "            \n",
    "            advantage = values - rewards \n",
    "            actorLoss = (-log_probs * advantage).mean()\n",
    "            criticLoss = 0.5 * advantage.pow(2).mean()\n",
    "            \n",
    "            loss = actorLoss + criticLoss\n",
    "            episodicAvgLoss += loss.item()/episodes\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(epoch, \":\", episodicAvgLoss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
