{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning-based Image Captioning with Embedding Reward\n",
    "Pranshu Gupta, Deep Learning @ Georgia Institute of Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from utils.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from utils.image_utils import image_from_url\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Working on: \", device)\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "max_seq_len = 17\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MS-COCO data\n",
    "We will use the Microsoft COCO dataset for captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
      "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
      "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
      "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
      "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
      "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
      "idx_to_word <class 'list'> 1004\n",
      "word_to_idx <class 'dict'> 1004\n",
      "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
      "val_urls <class 'numpy.ndarray'> (40504,) <U63\n",
      "train_captions_lens <class 'numpy.ndarray'> (400135,) float64\n",
      "val_captions_lens <class 'numpy.ndarray'> (195954,) float64\n"
     ]
    }
   ],
   "source": [
    "# Load COCO data from disk; this returns a dictionary\n",
    "# We'll work with dimensionality-reduced features for this notebook, but feel\n",
    "# free to experiment with the original features by changing the flag below.\n",
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "data[\"train_captions_lens\"] = np.zeros(data[\"train_captions\"].shape[0])\n",
    "data[\"val_captions_lens\"] = np.zeros(data[\"val_captions\"].shape[0])\n",
    "for i in range(data[\"train_captions\"].shape[0]):\n",
    "    data[\"train_captions_lens\"][i] = np.nonzero(data[\"train_captions\"][i] == 2)[0][0] + 1\n",
    "for i in range(data[\"val_captions\"].shape[0]):\n",
    "    data[\"val_captions_lens\"][i] = np.nonzero(data[\"val_captions\"][i] == 2)[0][0] + 1\n",
    "\n",
    "\n",
    "# Print out all the keys and values from the data dictionary\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = load_coco_data(max_train=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLEU_score(gt_caption, sample_caption):\n",
    "    \"\"\"\n",
    "    gt_caption: string, ground-truth caption\n",
    "    sample_caption: string, your model's predicted caption\n",
    "    Returns unigram BLEU score.\n",
    "    \"\"\"\n",
    "    reference = [x for x in gt_caption.split(' ') \n",
    "                 if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "    hypothesis = [x for x in sample_caption.split(' ') \n",
    "                  if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = [1])\n",
    "    return BLEUscore\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"\n",
    "    model: CaptioningRNN model\n",
    "    Prints unigram BLEU score averaged over 1000 training and val examples.\n",
    "    \"\"\"\n",
    "    BLEUscores = {}\n",
    "    for split in ['train', 'val']:\n",
    "        minibatch = sample_coco_minibatch(data, split=split, batch_size=1000)\n",
    "        gt_captions, features, urls = minibatch\n",
    "        gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n",
    "\n",
    "        sample_captions = model.sample(features)\n",
    "        sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n",
    "\n",
    "        total_score = 0.0\n",
    "        for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n",
    "            total_score += BLEU_score(gt_caption, sample_caption)\n",
    "\n",
    "        BLEUscores[split] = total_score / len(sample_captions)\n",
    "\n",
    "    for split in BLEUscores:\n",
    "        print('Average BLEU score for %s: %f' % (split, BLEUscores[split]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        \n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        \n",
    "        self.cnn2linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(wordvec_dim, hidden_dim, batch_first=True)\n",
    "        self.linear2vocab = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        hidden_init = self.cnn2linear(features)\n",
    "        cell_init = torch.zeros_like(hidden_init)\n",
    "        output, _ = self.lstm(input_captions, (hidden_init, cell_init))\n",
    "        output = self.linear2vocab(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardNetworkRNN(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(RewardNetworkRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.hidden_cell = torch.zeros(1, 1, self.hidden_dim).to(device)\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        self.gru = nn.GRU(wordvec_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        output, self.hidden_cell = self.gru(input_captions.view(len(input_captions) ,1, -1), self.hidden_cell)\n",
    "        return output\n",
    "    \n",
    "class RewardNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx):\n",
    "        super(RewardNetwork, self).__init__()\n",
    "        self.rewrnn = RewardNetworkRNN(word_to_idx)\n",
    "        self.visual_embed = nn.Linear(512, 512)\n",
    "        self.semantic_embed = nn.Linear(512, 512)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        for t in range(captions.shape[1]):\n",
    "            rrnn = self.rewrnn(captions[:, t])\n",
    "        rrnn = rrnn.squeeze(0).squeeze(1)\n",
    "        se = self.semantic_embed(rrnn)\n",
    "        ve = self.visual_embed(features)\n",
    "        return ve, se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetworkRNN(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(ValueNetworkRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.hidden_cell = (torch.zeros(1, 1, self.hidden_dim).to(device), torch.zeros(1, 1, self.hidden_dim).to(device))\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        self.lstm = nn.LSTM(wordvec_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        output, self.hidden_cell = self.lstm(input_captions.view(len(input_captions) ,1, -1), self.hidden_cell)\n",
    "        return output\n",
    "    \n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.valrnn = ValueNetworkRNN(word_to_idx)\n",
    "        self.linear1 = nn.Linear(1024, 512)\n",
    "        self.linear2 = nn.Linear(512, 1)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        for t in range(captions.shape[1]):\n",
    "            vrnn = self.valrnn(captions[:, t])\n",
    "        vrnn = vrnn.squeeze(0).squeeze(1)\n",
    "        state = torch.cat((features, vrnn), dim=1)\n",
    "        output = self.linear1(state)\n",
    "        output = self.linear2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValueNetwork(\n",
       "  (valrnn): ValueNetworkRNN(\n",
       "    (caption_embedding): Embedding(1004, 512)\n",
       "    (lstm): LSTM(512, 512)\n",
       "  )\n",
       "  (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "policyNet.load_state_dict(torch.load('models/policyNetwork.pt'))\n",
    "policyNet.train(mode=False)\n",
    "\n",
    "valueNet = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "valueNet.load_state_dict(torch.load('models/valueNetwork.pt'))\n",
    "valueNet.train(mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Caption Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCaptions(features, captions, model):\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    gen_caps = torch.tensor(captions[:, 0:1], device=device).long()\n",
    "    for t in range(max_seq_len-1):\n",
    "        output = model(features, gen_caps)\n",
    "        gen_caps = torch.cat((gen_caps, output[:, -1:, :].argmax(axis=2)), axis=1)\n",
    "    return gen_caps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search Caption Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCaptionsWithBeamSearch(features, captions, model, beamSize=5):\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    gen_caps = torch.tensor(captions[:, 0:1], device=device).long()\n",
    "    candidates = [(gen_caps, 0)]\n",
    "    for t in range(max_seq_len-1):\n",
    "        next_candidates = []\n",
    "        for c in range(len(candidates)):\n",
    "            output = model(features, candidates[c][0])\n",
    "            probs, words = torch.topk(output[:, -1:, :], beamSize)\n",
    "            for i in range(beamSize):\n",
    "                cap = torch.cat((candidates[c][0], words[:, :, i]), axis=1)\n",
    "                score = candidates[c][1] - torch.log(probs[0, 0, i]).item()\n",
    "                next_candidates.append((cap, score))\n",
    "        ordered_candidates = sorted(next_candidates, key=lambda tup:tup[1])\n",
    "        candidates = ordered_candidates[:beamSize]\n",
    "    return candidates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lookahead Inference with Policy and Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCaptionsWithBeamSearchValueScoring(features, captions, model, beamSize=5):\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    gen_caps = torch.tensor(captions[:, 0:1], device=device).long()\n",
    "    candidates = [(gen_caps, 0)]\n",
    "    for t in range(max_seq_len-1):\n",
    "        next_candidates = []\n",
    "        for c in range(len(candidates)):\n",
    "            output = model(features, candidates[c][0])\n",
    "            probs, words = torch.topk(output[:, -1:, :], beamSize)\n",
    "            for i in range(beamSize):\n",
    "                cap = torch.cat((candidates[c][0], words[:, :, i]), axis=1)\n",
    "                value = valueNet(features.squeeze(0), cap).detach()\n",
    "                score = candidates[c][1] - 0.6*value.item() -0.4*torch.log(probs[0, 0, i]).item()\n",
    "                next_candidates.append((cap, score))\n",
    "        ordered_candidates = sorted(next_candidates, key=lambda tup:tup[1])\n",
    "        candidates = ordered_candidates[:beamSize]\n",
    "    return candidates   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRUTH: <START> two female soccer players <UNK> over the ball in a match <END>\n",
      "GREEDY: <START> a group of baseball players in the field <END>\n",
      "\n",
      "TRUTH: <START> a man in a suit is outside in a yard <END>\n",
      "GREEDY: <START> a woman is <UNK> holding an umbrella <END>\n",
      "\n",
      "TRUTH: <START> a person leaning against a red wall with his hand to his face <END>\n",
      "GREEDY: <START> a young man wearing a suit and tie glasses <UNK> <END>\n",
      "\n",
      "TRUTH: <START> an image of park bench that <UNK> outside <END>\n",
      "GREEDY: <START> a park bench is sitting on a tree <END>\n",
      "\n",
      "TRUTH: <START> a man that is on a surfboard in the water <END>\n",
      "GREEDY: <START> a man riding a wave on a surfboard <END>\n",
      "\n",
      "TRUTH: <START> a <UNK> on the tracks in a country setting <END>\n",
      "GREEDY: <START> a group of cattle are standing next to each other <END>\n",
      "\n",
      "TRUTH: <START> a small kitchen has two silver sinks and stove <END>\n",
      "GREEDY: <START> a bathroom that has two toilets in a bowl <END>\n",
      "\n",
      "TRUTH: <START> the back side of a <UNK> riding a police motorcycle in a street <END>\n",
      "GREEDY: <START> a police officer is sitting on a motorcycle in front <END>\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    max_seq_len = 17\n",
    "    captions, features, urls = sample_coco_minibatch(small_data, batch_size=10, split='train')\n",
    "    for i in range(100):\n",
    "        gen_caps = []\n",
    "#         gen_caps.append(GenerateCaptions(features[i:i+1], captions[i:i+1], policyNet)[0])\n",
    "        gen_caps.append(GenerateCaptionsWithBeamSearch(features[i:i+1], captions[i:i+1], policyNet)[0][0][0])\n",
    "#         gen_caps.append(GenerateCaptionsWithBeamSearchValueScoring(features[i:i+1], captions[i:i+1], policyNet)[0][0][0])\n",
    "        decoded_tru_caps = decode_captions(captions[i], data[\"idx_to_word\"])\n",
    "    #     try:\n",
    "    #         plt.imshow(image_from_url(urls[i]))\n",
    "    #         plt.show()\n",
    "    #     except:\n",
    "    #         continue\n",
    "        print()\n",
    "        print(\"TRUTH:\", decoded_tru_caps)\n",
    "        decoded_gen_caps = decode_captions(gen_caps[0], data[\"idx_to_word\"])\n",
    "        print(\"GREEDY:\", decoded_gen_caps)\n",
    "#         decoded_gen_caps = decode_captions(gen_caps[1], data[\"idx_to_word\"])\n",
    "#         print(\"BEAM:\", decoded_gen_caps)\n",
    "#         decoded_gen_caps = decode_captions(gen_caps[2], data[\"idx_to_word\"])\n",
    "#         print(\"VALUE:\", decoded_gen_caps)\n",
    "    #     print(BLEU_score(decoded_tru_caps, decoded_gen_caps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "policyNetwork = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(policyNetwork.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: -0.7622961401939392\n",
      "epoch: 2 loss: -0.7967267632484436\n",
      "epoch: 3 loss: -0.8298563361167908\n",
      "epoch: 4 loss: -0.8488402962684631\n",
      "epoch: 7 loss: -0.8808352947235107\n",
      "epoch: 9 loss: -0.9272071123123169\n",
      "epoch: 10 loss: -0.9303421974182129\n",
      "epoch: 11 loss: -0.9743098020553589\n",
      "epoch: 12 loss: -0.9925467371940613\n",
      "epoch: 13 loss: -0.9987075924873352\n",
      "epoch: 14 loss: -1.038584589958191\n",
      "epoch: 15 loss: -1.0717610120773315\n",
      "epoch: 16 loss: -1.1022742986679077\n",
      "epoch: 17 loss: -1.1580274105072021\n",
      "epoch: 18 loss: -1.1924822330474854\n",
      "epoch: 19 loss: -1.2219786643981934\n",
      "epoch: 20 loss: -1.288925290107727\n",
      "epoch: 22 loss: -1.3553766012191772\n",
      "epoch: 23 loss: -1.3997516632080078\n",
      "epoch: 24 loss: -1.4628175497055054\n",
      "epoch: 25 loss: -1.4969642162322998\n",
      "epoch: 26 loss: -1.5474493503570557\n",
      "epoch: 27 loss: -1.5780303478240967\n",
      "epoch: 28 loss: -1.637071967124939\n",
      "epoch: 29 loss: -1.7035317420959473\n",
      "epoch: 30 loss: -1.7599382400512695\n",
      "epoch: 31 loss: -1.7624255418777466\n",
      "epoch: 32 loss: -1.79536771774292\n",
      "epoch: 33 loss: -1.9049057960510254\n",
      "epoch: 34 loss: -1.905509114265442\n",
      "epoch: 35 loss: -1.9422763586044312\n",
      "epoch: 36 loss: -1.9623534679412842\n",
      "epoch: 37 loss: -2.0559895038604736\n",
      "epoch: 39 loss: -2.0719637870788574\n",
      "epoch: 41 loss: -2.164222240447998\n",
      "epoch: 43 loss: -2.2094295024871826\n",
      "epoch: 44 loss: -2.224170446395874\n",
      "epoch: 47 loss: -2.263216733932495\n",
      "epoch: 48 loss: -2.447510242462158\n",
      "epoch: 55 loss: -2.4901914596557617\n",
      "epoch: 58 loss: -2.4983081817626953\n",
      "epoch: 61 loss: -2.624643564224243\n",
      "epoch: 65 loss: -2.66452956199646\n",
      "epoch: 68 loss: -2.7176859378814697\n",
      "epoch: 70 loss: -2.742976188659668\n",
      "epoch: 72 loss: -2.7827391624450684\n",
      "epoch: 73 loss: -2.812913417816162\n",
      "epoch: 76 loss: -2.932640552520752\n",
      "epoch: 79 loss: -2.940009117126465\n",
      "epoch: 80 loss: -2.976353168487549\n",
      "epoch: 81 loss: -2.993997573852539\n",
      "epoch: 84 loss: -3.0368900299072266\n",
      "epoch: 85 loss: -3.2221548557281494\n",
      "epoch: 89 loss: -3.284658432006836\n",
      "epoch: 90 loss: -3.2855236530303955\n",
      "epoch: 92 loss: -3.311516284942627\n",
      "epoch: 94 loss: -3.339980125427246\n",
      "epoch: 95 loss: -3.467028856277466\n",
      "epoch: 96 loss: -3.5020976066589355\n",
      "epoch: 100 loss: -3.534869909286499\n",
      "epoch: 102 loss: -3.5671491622924805\n",
      "epoch: 104 loss: -3.645078659057617\n",
      "epoch: 105 loss: -3.791559934616089\n",
      "epoch: 115 loss: -3.8685455322265625\n",
      "epoch: 117 loss: -3.918956756591797\n",
      "epoch: 118 loss: -3.9265329837799072\n",
      "epoch: 124 loss: -4.069361209869385\n",
      "epoch: 126 loss: -4.180495738983154\n",
      "epoch: 127 loss: -4.2691426277160645\n",
      "epoch: 139 loss: -4.322033405303955\n",
      "epoch: 141 loss: -4.47972297668457\n",
      "epoch: 150 loss: -4.5200419425964355\n",
      "epoch: 153 loss: -4.589718818664551\n",
      "epoch: 154 loss: -4.62755823135376\n",
      "epoch: 156 loss: -4.719480514526367\n",
      "epoch: 170 loss: -4.794101238250732\n",
      "epoch: 171 loss: -5.021941184997559\n",
      "epoch: 184 loss: -5.177505016326904\n",
      "epoch: 189 loss: -5.37017297744751\n",
      "epoch: 203 loss: -5.410757064819336\n",
      "epoch: 216 loss: -5.418837070465088\n",
      "epoch: 218 loss: -5.4269585609436035\n",
      "epoch: 220 loss: -5.504226207733154\n",
      "epoch: 224 loss: -5.747720718383789\n",
      "epoch: 226 loss: -5.78240966796875\n",
      "epoch: 250 loss: -5.872592449188232\n",
      "epoch: 254 loss: -5.963210582733154\n",
      "epoch: 268 loss: -6.076025485992432\n",
      "epoch: 278 loss: -6.142159938812256\n",
      "epoch: 288 loss: -6.163835048675537\n",
      "epoch: 295 loss: -6.232791900634766\n",
      "epoch: 299 loss: -6.2635416984558105\n",
      "epoch: 302 loss: -6.46587610244751\n",
      "epoch: 324 loss: -6.500588417053223\n",
      "epoch: 329 loss: -6.642949104309082\n",
      "epoch: 345 loss: -6.855987071990967\n",
      "epoch: 403 loss: -6.927675724029541\n",
      "epoch: 433 loss: -6.990056991577148\n",
      "epoch: 441 loss: -7.086748123168945\n",
      "epoch: 460 loss: -7.105680465698242\n",
      "epoch: 467 loss: -7.128045558929443\n",
      "epoch: 472 loss: -7.151787757873535\n",
      "epoch: 481 loss: -7.171787261962891\n",
      "epoch: 483 loss: -7.223944187164307\n",
      "epoch: 491 loss: -7.298123836517334\n",
      "epoch: 506 loss: -7.460268020629883\n",
      "epoch: 537 loss: -7.490850925445557\n",
      "epoch: 541 loss: -7.519249439239502\n",
      "epoch: 556 loss: -7.671548366546631\n",
      "epoch: 574 loss: -7.688333511352539\n",
      "epoch: 619 loss: -7.898252964019775\n",
      "epoch: 716 loss: -7.900803565979004\n",
      "epoch: 761 loss: -7.905142784118652\n",
      "epoch: 771 loss: -8.006925582885742\n",
      "epoch: 813 loss: -8.014798164367676\n",
      "epoch: 819 loss: -8.209362030029297\n",
      "epoch: 860 loss: -8.310221672058105\n",
      "epoch: 992 loss: -8.465163230895996\n",
      "epoch: 1084 loss: -8.596484184265137\n",
      "epoch: 1188 loss: -8.613061904907227\n",
      "epoch: 1233 loss: -8.734881401062012\n",
      "epoch: 1317 loss: -8.735031127929688\n",
      "epoch: 1480 loss: -9.012925148010254\n",
      "epoch: 1628 loss: -9.070869445800781\n",
      "epoch: 1666 loss: -9.382979393005371\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-8620afb73f40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pranshu/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pranshu/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "bestLoss = 10000 \n",
    "#0.006700546946376562\n",
    "\n",
    "for epoch in range(50000):\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    captions_in = torch.tensor(captions[:, :-1], device=device).long()\n",
    "    captions_ou = torch.tensor(captions[:, 1:], device=device).long()\n",
    "    output = policyNetwork(features, captions_in)\n",
    "    \n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        caplen = np.nonzero(captions[i] == 2)[0][0] + 1\n",
    "        loss += (caplen/batch_size)*criterion(output[i][:caplen], captions_ou[i][:caplen])\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        torch.save(policyNetwork.state_dict(), \"policyNetwork.pt\")\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardNetwork = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "optimizer = optim.Adam(rewardNetwork.parameters(), lr=0.001)\n",
    "\n",
    "# https://cs230-stanford.github.io/pytorch-nlp.html#writing-a-custom-loss-function\n",
    "def VisualSemanticEmbeddingLoss(visuals, semantics):\n",
    "    beta = 0.002\n",
    "    N, D = visuals.shape\n",
    "    \n",
    "    visloss = torch.mm(visuals, semantics.t())\n",
    "    visloss = visloss - torch.diag(visloss).unsqueeze(1)\n",
    "    visloss = visloss + (beta/N)*(torch.ones((N, N)).to(device) - torch.eye(N).to(device))\n",
    "    visloss = F.relu(visloss)\n",
    "    visloss = torch.sum(visloss)/N\n",
    "    \n",
    "    semloss = torch.mm(semantics, visuals.t())\n",
    "    semloss = semloss - torch.diag(semloss).unsqueeze(1)\n",
    "    semloss = semloss + (beta/N)*(torch.ones((N, N)).to(device) - torch.eye(N).to(device))\n",
    "    semloss = F.relu(semloss)\n",
    "    semloss = torch.sum(semloss)/N\n",
    "    \n",
    "    return visloss + semloss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 153.8406219482422\n",
      "epoch: 1 loss: 145.78004455566406\n",
      "epoch: 2 loss: 144.26637268066406\n",
      "epoch: 5 loss: 127.2883529663086\n",
      "epoch: 17 loss: 126.70248413085938\n",
      "epoch: 21 loss: 122.92036437988281\n",
      "epoch: 45 loss: 117.26567077636719\n",
      "epoch: 48 loss: 109.99120330810547\n",
      "epoch: 56 loss: 105.07620239257812\n",
      "epoch: 183 loss: 104.64308166503906\n",
      "epoch: 198 loss: 96.82013702392578\n",
      "epoch: 209 loss: 94.6942138671875\n",
      "epoch: 275 loss: 93.03327178955078\n",
      "epoch: 286 loss: 86.01957702636719\n",
      "epoch: 345 loss: 83.19971466064453\n",
      "epoch: 402 loss: 82.8239974975586\n",
      "epoch: 447 loss: 80.05474853515625\n",
      "epoch: 450 loss: 66.36756134033203\n",
      "epoch: 589 loss: 53.008888244628906\n",
      "epoch: 825 loss: 52.94926452636719\n",
      "epoch: 964 loss: 52.840293884277344\n",
      "epoch: 965 loss: 52.49498748779297\n",
      "epoch: 974 loss: 51.35733413696289\n",
      "epoch: 983 loss: 49.916099548339844\n",
      "epoch: 984 loss: 48.341064453125\n",
      "epoch: 1031 loss: 46.65217590332031\n",
      "epoch: 1099 loss: 44.16111373901367\n",
      "epoch: 1141 loss: 41.64582061767578\n",
      "epoch: 1358 loss: 39.973663330078125\n",
      "epoch: 1380 loss: 39.9705924987793\n",
      "epoch: 1407 loss: 35.39472961425781\n",
      "epoch: 1457 loss: 31.444091796875\n",
      "epoch: 1646 loss: 31.429134368896484\n",
      "epoch: 1727 loss: 31.133546829223633\n",
      "epoch: 1755 loss: 30.463123321533203\n",
      "epoch: 1759 loss: 26.788545608520508\n",
      "epoch: 1827 loss: 26.042255401611328\n",
      "epoch: 1842 loss: 21.898014068603516\n",
      "epoch: 2187 loss: 21.157331466674805\n",
      "epoch: 2309 loss: 18.28589630126953\n",
      "epoch: 2390 loss: 15.919644355773926\n",
      "epoch: 2624 loss: 15.880849838256836\n",
      "epoch: 2631 loss: 15.8115234375\n",
      "epoch: 2673 loss: 14.645584106445312\n",
      "epoch: 2750 loss: 14.125833511352539\n",
      "epoch: 3006 loss: 11.71585464477539\n",
      "epoch: 3238 loss: 11.29970645904541\n",
      "epoch: 3244 loss: 11.111616134643555\n",
      "epoch: 3426 loss: 10.99402904510498\n",
      "epoch: 3429 loss: 10.969250679016113\n",
      "epoch: 3447 loss: 10.89189624786377\n",
      "epoch: 3544 loss: 10.67943286895752\n",
      "epoch: 3550 loss: 10.490233421325684\n",
      "epoch: 3559 loss: 10.176025390625\n",
      "epoch: 3590 loss: 9.266550064086914\n",
      "epoch: 3666 loss: 9.090851783752441\n",
      "epoch: 3719 loss: 8.848204612731934\n",
      "epoch: 3733 loss: 8.70340347290039\n",
      "epoch: 3737 loss: 8.654035568237305\n",
      "epoch: 3761 loss: 8.474893569946289\n",
      "epoch: 3769 loss: 8.241154670715332\n",
      "epoch: 3944 loss: 8.151841163635254\n",
      "epoch: 3980 loss: 7.570710182189941\n",
      "epoch: 4069 loss: 7.558771133422852\n",
      "epoch: 4177 loss: 6.95650577545166\n",
      "epoch: 4196 loss: 6.181880474090576\n",
      "epoch: 4202 loss: 5.635531902313232\n",
      "epoch: 4623 loss: 5.336463451385498\n",
      "epoch: 4853 loss: 5.308542251586914\n",
      "epoch: 4901 loss: 4.913893222808838\n",
      "epoch: 4995 loss: 4.310166835784912\n",
      "epoch: 5045 loss: 4.2184271812438965\n",
      "epoch: 5060 loss: 3.9159979820251465\n",
      "epoch: 5089 loss: 3.868506669998169\n",
      "epoch: 5298 loss: 3.447890281677246\n",
      "epoch: 5555 loss: 3.4005496501922607\n",
      "epoch: 5585 loss: 3.276308536529541\n",
      "epoch: 5679 loss: 3.0235280990600586\n",
      "epoch: 5724 loss: 3.013934850692749\n",
      "epoch: 5751 loss: 2.8977320194244385\n",
      "epoch: 5876 loss: 2.62048602104187\n",
      "epoch: 6221 loss: 2.5130882263183594\n",
      "epoch: 6296 loss: 2.1700332164764404\n",
      "epoch: 6322 loss: 2.0207254886627197\n",
      "epoch: 6832 loss: 1.798125982284546\n",
      "epoch: 7072 loss: 1.671970248222351\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "bestLoss = 10000\n",
    "\n",
    "for epoch in range(50000):\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float()\n",
    "    captions = torch.tensor(captions, device=device).long()\n",
    "    ve, se = rewardNetwork(features, captions)\n",
    "    loss = VisualSemanticEmbeddingLoss(ve, se)\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        torch.save(rewardNetwork.state_dict(), \"rewardNetwork.pt\")\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    rewardNetwork.rewrnn.hidden_cell.detach_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRewards(features, captions, model):\n",
    "    visEmbeds, semEmbeds = model(features, captions)\n",
    "    visEmbeds = F.normalize(visEmbeds, p=2, dim=1) \n",
    "    semEmbeds = F.normalize(semEmbeds, p=2, dim=1) \n",
    "    rewards = torch.sum(visEmbeds*semEmbeds, axis=1).unsqueeze(1)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RewardNetwork(\n",
      "  (rewrnn): RewardNetworkRNN(\n",
      "    (caption_embedding): Embedding(1004, 512)\n",
      "    (gru): GRU(512, 512)\n",
      "  )\n",
      "  (visual_embed): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (semantic_embed): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "PolicyNetwork(\n",
      "  (caption_embedding): Embedding(1004, 512)\n",
      "  (cnn2linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (lstm): LSTM(512, 512, batch_first=True)\n",
      "  (linear2vocab): Linear(in_features=512, out_features=1004, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ValueNetwork(\n",
       "  (valrnn): ValueNetworkRNN(\n",
       "    (caption_embedding): Embedding(1004, 512)\n",
       "    (lstm): LSTM(512, 512)\n",
       "  )\n",
       "  (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewardNet = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "rewardNet.load_state_dict(torch.load('models/rewardNetwork.pt'))\n",
    "for param in rewardNet.parameters():\n",
    "    param.require_grad = False\n",
    "print(rewardNet)\n",
    "\n",
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "policyNet.load_state_dict(torch.load('models/policyNetwork.pt'))\n",
    "for param in policyNet.parameters():\n",
    "    param.require_grad = False\n",
    "print(policyNet)\n",
    "\n",
    "valueNetwork = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(valueNetwork.parameters(), lr=0.0001)\n",
    "valueNetwork.train(mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patcha/anaconda3/envs/pranshu/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 512]) torch.Size([50, 1])\n",
      "epoch: 0 loss: 0.12978266179561615\n",
      "torch.Size([50, 512]) torch.Size([50, 2])\n",
      "epoch: 1 loss: 0.11661884188652039\n",
      "torch.Size([50, 512]) torch.Size([50, 2])\n",
      "epoch: 2 loss: 0.10857027769088745\n",
      "torch.Size([50, 512]) torch.Size([50, 6])\n",
      "epoch: 3 loss: 0.07581345736980438\n",
      "torch.Size([50, 512]) torch.Size([50, 5])\n",
      "torch.Size([50, 512]) torch.Size([50, 2])\n",
      "epoch: 5 loss: 0.07159502804279327\n",
      "torch.Size([50, 512]) torch.Size([50, 14])\n",
      "torch.Size([50, 512]) torch.Size([50, 8])\n",
      "torch.Size([50, 512]) torch.Size([50, 16])\n",
      "torch.Size([50, 512]) torch.Size([50, 9])\n",
      "torch.Size([50, 512]) torch.Size([50, 8])\n",
      "torch.Size([50, 512]) torch.Size([50, 16])\n",
      "torch.Size([50, 512]) torch.Size([50, 11])\n",
      "torch.Size([50, 512]) torch.Size([50, 2])\n",
      "epoch: 13 loss: 0.06482134759426117\n",
      "torch.Size([50, 512]) torch.Size([50, 13])\n",
      "epoch: 14 loss: 0.053911302238702774\n",
      "torch.Size([50, 512]) torch.Size([50, 10])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-8dfb378caa88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbestLoss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mbestLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalueNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"valueNetwork.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "bestLoss = 10000\n",
    "max_seq_len = 17\n",
    "\n",
    "for epoch in range(50000):\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float()\n",
    "    \n",
    "    # Generate captions using the policy network\n",
    "    captions = GenerateCaptions(features, captions, policyNet)\n",
    "    \n",
    "    # Compute the reward of the generated caption using reward network\n",
    "    rewards = GetRewards(features, captions, rewardNet)\n",
    "    \n",
    "    # Compute the value of a random state in the generation process\n",
    "    print(features.shape, captions[:, :random.randint(1, 17)].shape)\n",
    "    values = valueNetwork(features, captions[:, :random.randint(1, 17)])\n",
    "    \n",
    "    # Compute the loss for the value and the reward\n",
    "    loss = criterion(values, rewards)\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        torch.save(valueNetwork.state_dict(), \"valueNetwork.pt\")\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    valueNetwork.valrnn.hidden_cell[0].detach_()\n",
    "    valueNetwork.valrnn.hidden_cell[1].detach_()\n",
    "    rewardNet.rewrnn.hidden_cell.detach_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "Advantage Actor Critic Model for Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvantageActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, valueNet, policyNet):\n",
    "        super(AdvantageActorCriticNetwork, self).__init__()\n",
    "\n",
    "        self.valueNet = valueNet #RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "        self.policyNet = policyNet #PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # Get value from value network\n",
    "        values = self.valueNet(features, captions)\n",
    "        # Get action probabilities from policy network\n",
    "        probs = self.policyNet(features.unsqueeze(0), captions)[:, -1:, :]        \n",
    "        return values, probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardNet = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "valueNet = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "\n",
    "rewardNet.load_state_dict(torch.load('models/rewardNetwork.pt'))\n",
    "policyNet.load_state_dict(torch.load('models/policyNetwork.pt'))\n",
    "valueNet.load_state_dict(torch.load('models/valueNetwork.pt'))\n",
    "\n",
    "a2cNetwork = AdvantageActorCriticNetwork(valueNet, policyNet)\n",
    "optimizer = optim.Adam(a2cNetwork.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.02320903023704886\n",
      "1 : 0.01312393933010753\n",
      "2 : 0.00771524380070332\n",
      "3 : -0.007593203229771461\n",
      "4 : -0.001424596389406362\n",
      "5 : 0.00989914114077692\n",
      "6 : 0.005231798512540989\n",
      "7 : 0.006629595400008839\n",
      "8 : 0.012109559222590178\n",
      "9 : -0.0006239669210481226\n",
      "10 : 0.009723953194916247\n",
      "11 : 0.0044474254810484126\n",
      "12 : 0.008023221267485496\n",
      "13 : 0.0056188294969615514\n",
      "14 : 0.003599554880929643\n",
      "15 : 0.005732270738044463\n",
      "16 : 0.0060812147264368835\n",
      "17 : 0.004769039509592402\n",
      "18 : 0.005961071758065374\n",
      "19 : 0.004842769111783126\n",
      "20 : 0.004504630964001989\n",
      "21 : 0.005190590277634326\n",
      "22 : 0.005683154103317063\n",
      "23 : 0.0034073841822100793\n",
      "24 : 0.008774827713496051\n",
      "25 : 0.00917623785826436\n",
      "26 : 0.004189597215736285\n",
      "27 : -0.0027697008736140573\n",
      "28 : 0.005185449457094364\n",
      "29 : 0.0045179704393376605\n",
      "30 : 0.0042511611874215295\n",
      "31 : -0.0078844551929069\n",
      "32 : 0.007009633950765419\n",
      "33 : 0.010297190369747113\n",
      "34 : 0.004855246039951453\n",
      "35 : 0.004842445416725242\n",
      "36 : 0.0050522680560970935\n",
      "37 : 0.005479029436974087\n",
      "38 : 0.007918740897948738\n",
      "39 : 0.0022630827387547466\n",
      "40 : 0.008102019236112026\n",
      "41 : -0.0003890461256378336\n",
      "42 : 0.004853793641377706\n",
      "43 : 0.00518377476662863\n",
      "44 : 0.04568159836073391\n",
      "45 : 0.0037171617783315018\n",
      "46 : 0.0250095325850998\n",
      "47 : 0.0005596457817591735\n",
      "48 : -0.016179781447863206\n",
      "49 : 0.007948643692070613\n",
      "50 : 0.015325917979353103\n",
      "51 : -0.0041235365193188055\n",
      "52 : 0.002310697392167639\n",
      "53 : 0.00371338918164838\n",
      "54 : 0.008961729694019597\n",
      "55 : -0.009313685367669676\n",
      "56 : 0.005954324267295306\n",
      "57 : 0.0056331735115963955\n",
      "58 : 0.004532474799343617\n",
      "59 : 0.006746807880117557\n",
      "60 : 0.018762081986205884\n",
      "61 : 0.0024649349548053583\n",
      "62 : 0.006535298603048431\n",
      "63 : 0.0031438050331780687\n",
      "64 : 0.005512421529638233\n",
      "65 : -0.005054478847741847\n",
      "66 : 0.0071869758896355085\n",
      "67 : 0.0041420101764379075\n",
      "68 : 0.00884173417274724\n",
      "69 : 0.0063623671869572716\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-feec9e4e943a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGetRewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewardNet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-e7a2df754ed3>\u001b[0m in \u001b[0;36mGetRewards\u001b[0;34m(features, captions, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mGetRewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mvisEmbeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemEmbeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mvisEmbeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisEmbeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msemEmbeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemEmbeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisEmbeds\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msemEmbeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pranshu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c30aa1131594>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, captions)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mrrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mrrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemantic_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrrnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "curriculum = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "episodes = 50\n",
    "\n",
    "small_data = load_coco_data(max_train=50000)\n",
    "\n",
    "for level in curriculum:\n",
    "    \n",
    "    for epoch in range(1000):        \n",
    "        episodicAvgLoss = 0\n",
    "        \n",
    "        captions, features, _ = sample_coco_minibatch(small_data, batch_size=episodes, split='train')\n",
    "        features = torch.tensor(features, device=device).float()\n",
    "        captions = torch.tensor(captions, device=device).long()\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            log_probs = []\n",
    "            values = []\n",
    "            rewards = []\n",
    "            caplen = np.nonzero(captions[episode] == 2)[0][0] + 1\n",
    "            \n",
    "            if (caplen - level > 1):\n",
    "                captions_in = captions[episode:episode+1, :caplen-level]\n",
    "                features_in = features[episode:episode+1]\n",
    "\n",
    "                for step in range(level):\n",
    "                    value, probs = a2cNetwork(features_in, captions_in)\n",
    "                    probs = F.softmax(probs, dim=2)\n",
    "                    \n",
    "                    dist = probs.cpu().detach().numpy()[0,0]\n",
    "                    action = np.random.choice(probs.shape[-1], p=dist)\n",
    "                    \n",
    "                    gen_cap = torch.from_numpy(np.array([action])).unsqueeze(0).to(device)\n",
    "                    captions_in = torch.cat((captions_in, gen_cap), axis=1)\n",
    "                    \n",
    "                    log_prob = torch.log(probs[0, 0, action])\n",
    "                    \n",
    "                    reward = GetRewards(features_in, captions_in, rewardNet)\n",
    "                    reward = reward.cpu().detach().numpy()[0, 0]\n",
    "                    \n",
    "                    rewards.append(reward)\n",
    "                    values.append(value)\n",
    "                    log_probs.append(log_prob)\n",
    "                    \n",
    "            values = torch.FloatTensor(values).to(device)\n",
    "            rewards = torch.FloatTensor(rewards).to(device)\n",
    "            log_probs = torch.stack(log_probs).to(device)\n",
    "            \n",
    "            advantage = values - rewards \n",
    "            actorLoss = (-log_probs * advantage).mean()\n",
    "            criticLoss = 0.5 * advantage.pow(2).mean()\n",
    "            \n",
    "            loss = actorLoss + criticLoss\n",
    "            episodicAvgLoss += loss.item()/episodes\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(epoch, \":\", episodicAvgLoss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
